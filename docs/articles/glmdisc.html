<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>The `glmdisc` package: discretization at its finest • glmdisc</title>
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/css/bootstrap.min.css" integrity="sha256-bZLfwXAP04zRMK2BjiO8iu9pf4FbLqX6zitd+tIvLhE=" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><meta property="og:title" content="The `glmdisc` package: discretization at its finest">
<meta property="og:description" content="glmdisc">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body data-spy="scroll" data-target="#toc">
    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">glmdisc</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="Released version">0.6</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../index.html">
    <span class="fas fa fas fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../articles/glmdisc.html">Get started</a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/adimajo/glmdisc/">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>The <code>glmdisc</code> package: discretization at its finest</h1>
                        <h4 class="author">Adrien Ehrhardt</h4>
            
            <h4 class="date">2020-08-26</h4>
      
      <small class="dont-index">Source: <a href="https://github.com/adimajo/glmdisc/blob/master/vignettes/glmdisc.Rmd"><code>vignettes/glmdisc.Rmd</code></a></small>
      <div class="hidden name"><code>glmdisc.Rmd</code></div>

    </div>

    
    
<div id="context" class="section level1">
<h1 class="hasAnchor">
<a href="#context" class="anchor"></a>Context</h1>
<p>This research has been financed by Crédit Agricole Consumer Finance (CA CF), subsidiary of the Crédit Agricole Group which provides all kinds of banking and insurance services. CA CF specializes in consumer loans. It is a joint work at <a href="https://www.inria.fr/en/centre/lille">Inria Nord-Europe</a> between Adrien Ehrhardt (CA CF, Inria), Christophe Biernacki (Inria, Lille University), Vincent Vandewalle (Inria, Lille University) and Philippe Heinrich (Lille University).</p>
<p>In order to accept / reject loan applications more efficiently (both quicker and to select better applicants), most financial institutions resort to Credit Scoring: given the applicant’s characteristics he/she is given a Credit Score, which has been statistically designed using previously accepted applicants, and which partly decides whether the financial institution will grant the loan or not.</p>
<p>The current methodology for building a Credit Score in most financial institutions is based on logistic regression for several (mostly practical) reasons: it generally gives satisfactory discriminant results, it is reasonably well explainable (contrary to a random forest for example), it is robust to missing data (in particular not financed clients) that follow a MAR missingness mechanism.</p>
<div id="example" class="section level2">
<h2 class="hasAnchor">
<a href="#example" class="anchor"></a>Example</h2>
<p>In practice, the statistical modeler has historical data about each customer’s characteristics. For obvious reasons, only data available at the time of inquiry must be used to build a future application scorecard. Those data often take the form of a well-structured table with one line per client alongside their performance (did they pay back their loan or not?) as can be seen in the following table:</p>
<table class="table">
<thead><tr class="header">
<th align="left">Job</th>
<th align="left">Habitation</th>
<th align="right">Time_in_job</th>
<th align="right">Children</th>
<th align="left">Family_status</th>
<th align="left">Default</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">Craftsman</td>
<td align="left">Owner</td>
<td align="right">10</td>
<td align="right">0</td>
<td align="left">Divorced</td>
<td align="left">No</td>
</tr>
<tr class="even">
<td align="left">Technician</td>
<td align="left">Renter</td>
<td align="right">20</td>
<td align="right">1</td>
<td align="left">Widower</td>
<td align="left">No</td>
</tr>
<tr class="odd">
<td align="left">Executive</td>
<td align="left">Starter</td>
<td align="right">5</td>
<td align="right">2</td>
<td align="left">Single</td>
<td align="left">Yes</td>
</tr>
<tr class="even">
<td align="left">Office employee</td>
<td align="left">By family</td>
<td align="right">2</td>
<td align="right">3</td>
<td align="left">Married</td>
<td align="left">No</td>
</tr>
</tbody>
</table>
</div>
<div id="notations" class="section level2">
<h2 class="hasAnchor">
<a href="#notations" class="anchor"></a>Notations</h2>
<p>In the rest of the vignette, the random vector <span class="math inline">\(X=(X_j)_1^d\)</span> will designate the predictive features, i.e. the characteristics of a client. The random variable <span class="math inline">\(Y \in \{0,1\}\)</span> will designate the label, i.e. if the client has defaulted (<span class="math inline">\(Y=1\)</span>) or not (<span class="math inline">\(Y=0\)</span>).</p>
<p>We are provided with an i.i.d. sample <span class="math inline">\((\bar{x},\bar{y}) = (x_i,y_i)_1^n\)</span> consisting in <span class="math inline">\(n\)</span> observations of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.</p>
</div>
<div id="logistic-regression" class="section level2">
<h2 class="hasAnchor">
<a href="#logistic-regression" class="anchor"></a>Logistic regression</h2>
<p>The logistic regression model assumes the following relation between <span class="math inline">\(X\)</span> (supposed continuous here) and <span class="math inline">\(Y\)</span>: <span class="math display">\[\ln \left( \frac{p_\theta(Y=1|x)}{p_\theta(Y=0|x)} \right) = \theta_0 + \sum_{j=1}^d \theta_j*x_j  \]</span> where <span class="math inline">\(\theta = (\theta_j)_0^d\)</span> are estimated using <span class="math inline">\((\bar{x},\bar{y})\)</span>.</p>
<p>Clearly, the model assumes linearity of the logit transform of the response <span class="math inline">\(Y\)</span> with respect to <span class="math inline">\(X\)</span>.</p>
</div>
<div id="common-problems-with-logistic-regression-on-raw-data" class="section level2">
<h2 class="hasAnchor">
<a href="#common-problems-with-logistic-regression-on-raw-data" class="anchor"></a>Common problems with logistic regression on “raw” data</h2>
<p>Fitting a logistic regression model on “raw” data presents several problems, among which some are tackled here.</p>
<div id="feature-selection" class="section level3">
<h3 class="hasAnchor">
<a href="#feature-selection" class="anchor"></a>Feature selection</h3>
<p>First, among all collected information on individuals, some are irrelevant for predicting <span class="math inline">\(Y\)</span>. Their coefficient <span class="math inline">\(\theta_j\)</span> should subsequently be <span class="math inline">\(0\)</span> which might (eventually) be the case asymptotically (i.e. <span class="math inline">\(n \rightarrow \infty\)</span>).</p>
<p>Second, some collected information are highly correlated and affect each other’s coefficient estimation.</p>
<p>As a consequence, data scientists often perform feature selection before training a machine learning algorithm such as logistic regression.</p>
<p>There already exists methods and packages to perform feature selection, see for example the <code>caret</code> package.</p>
<p><code>glmdisc</code> is not a feature selection tool but acts as such as a side-effect as we will see in the next part.</p>
</div>
<div id="linearity" class="section level3">
<h3 class="hasAnchor">
<a href="#linearity" class="anchor"></a>Linearity</h3>
<p>When provided with continuous features, the logistic regression model assumes linearity of the logit transform of the response <span class="math inline">\(Y\)</span> with respect to <span class="math inline">\(X\)</span>. This might not be the case at all.</p>
<p>For example, we can simulate a logistic model with an arbitrary power of <span class="math inline">\(X\)</span> and then try to fit a linear logistic model:</p>
<div class="sourceCode" id="cb1"><html><body><pre class="r"><span class="no">x</span> <span class="kw">=</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span>(<span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span>(<span class="fl">1000</span>), <span class="kw">nrow</span> <span class="kw">=</span> <span class="fl">1000</span>, <span class="kw">ncol</span> <span class="kw">=</span> <span class="fl">1</span>)
<span class="no">p</span> <span class="kw">=</span> <span class="fl">1</span>/(<span class="fl">1</span>+<span class="fu"><a href="https://rdrr.io/r/base/Log.html">exp</a></span>(-<span class="fl">3</span>*<span class="no">x</span>^<span class="fl">5</span>))
<span class="no">y</span> <span class="kw">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">rbinom</a></span>(<span class="fl">1000</span>,<span class="fl">1</span>,<span class="no">p</span>)
<span class="no">modele_lin</span> <span class="kw">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/glm.html">glm</a></span>(<span class="no">y</span> ~ <span class="no">x</span>, <span class="kw">family</span> <span class="kw">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/family.html">binomial</a></span>(<span class="kw">link</span><span class="kw">=</span><span class="st">"logit"</span>))
<span class="no">pred_lin</span> <span class="kw">&lt;-</span> <span class="fu"><a href="../reference/predict.html">predict</a></span>(<span class="no">modele_lin</span>,<span class="fu"><a href="https://rdrr.io/r/base/as.data.frame.html">as.data.frame</a></span>(<span class="no">x</span>),<span class="kw">type</span><span class="kw">=</span><span class="st">"response"</span>)
<span class="no">pred_lin_logit</span> <span class="kw">&lt;-</span> <span class="fu"><a href="../reference/predict.html">predict</a></span>(<span class="no">modele_lin</span>,<span class="fu"><a href="https://rdrr.io/r/base/as.data.frame.html">as.data.frame</a></span>(<span class="no">x</span>))</pre></body></html></div>
<table class="table">
<thead><tr class="header">
<th align="right">True_prob</th>
<th align="right">Pred_lin</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="right">0.5007289</td>
<td align="right">0.4865701</td>
</tr>
<tr class="even">
<td align="right">0.5001627</td>
<td align="right">0.4541247</td>
</tr>
<tr class="odd">
<td align="right">0.8171524</td>
<td align="right">0.7676885</td>
</tr>
<tr class="even">
<td align="right">0.5587921</td>
<td align="right">0.6580008</td>
</tr>
<tr class="odd">
<td align="right">0.5926925</td>
<td align="right">0.6838867</td>
</tr>
<tr class="even">
<td align="right">0.9215439</td>
<td align="right">0.7987942</td>
</tr>
</tbody>
</table>
<p>Of course, providing the <code>glm</code> function with a <code>formula</code> object containing <span class="math inline">\(X^5\)</span> would solve the problem. This can’t be done in practice for two reasons: first, it is too time-consuming to examine all features and candidate polynomials; second, we lose the interpretability of the logistic decision function which was of primary interest.</p>
<p>Consequently, we wish to discretize the input variable <span class="math inline">\(X\)</span> into a categorical feature which will “minimize” the error with respect to the “true” underlying relation:</p>
<div class="sourceCode" id="cb2"><html><body><pre class="r"><span class="no">x_disc</span> <span class="kw">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html">factor</a></span>(<span class="fu"><a href="https://rdrr.io/r/base/cut.html">cut</a></span>(<span class="no">x</span>,<span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span>(-<span class="fl">Inf</span>,<span class="fl">0.5</span>,<span class="fl">0.7</span>,<span class="fl">0.8</span>,<span class="fl">0.9</span>,+<span class="fl">Inf</span>)),<span class="kw">labels</span> <span class="kw">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span>(<span class="fl">1</span>,<span class="fl">2</span>,<span class="fl">3</span>,<span class="fl">4</span>,<span class="fl">5</span>))
<span class="no">modele_disc</span> <span class="kw">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/glm.html">glm</a></span>(<span class="no">y</span> ~ <span class="no">x_disc</span>, <span class="kw">family</span> <span class="kw">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/family.html">binomial</a></span>(<span class="kw">link</span><span class="kw">=</span><span class="st">"logit"</span>))
<span class="no">pred_disc</span> <span class="kw">&lt;-</span> <span class="fu"><a href="../reference/predict.html">predict</a></span>(<span class="no">modele_disc</span>,<span class="fu"><a href="https://rdrr.io/r/base/as.data.frame.html">as.data.frame</a></span>(<span class="no">x_disc</span>),<span class="kw">type</span><span class="kw">=</span><span class="st">"response"</span>)
<span class="no">pred_disc_logit</span> <span class="kw">&lt;-</span> <span class="fu"><a href="../reference/predict.html">predict</a></span>(<span class="no">modele_disc</span>,<span class="fu"><a href="https://rdrr.io/r/base/as.data.frame.html">as.data.frame</a></span>(<span class="no">x_disc</span>))</pre></body></html></div>
<table class="table">
<thead><tr class="header">
<th align="right">True_prob</th>
<th align="right">Pred_lin</th>
<th align="right">Pred_disc</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="right">0.5007289</td>
<td align="right">0.4865701</td>
<td align="right">0.4861111</td>
</tr>
<tr class="even">
<td align="right">0.5001627</td>
<td align="right">0.4541247</td>
<td align="right">0.4861111</td>
</tr>
<tr class="odd">
<td align="right">0.8171524</td>
<td align="right">0.7676885</td>
<td align="right">0.7956989</td>
</tr>
<tr class="even">
<td align="right">0.5587921</td>
<td align="right">0.6580008</td>
<td align="right">0.5754717</td>
</tr>
<tr class="odd">
<td align="right">0.5926925</td>
<td align="right">0.6838867</td>
<td align="right">0.5754717</td>
</tr>
<tr class="even">
<td align="right">0.9215439</td>
<td align="right">0.7987942</td>
<td align="right">0.9306931</td>
</tr>
</tbody>
</table>
<p><img src="glmdisc_files/figure-html/unnamed-chunk-5-1.png" width="672" style="display: block; margin: auto;"></p>
</div>
<div id="too-many-values-per-categorical-feature" class="section level3">
<h3 class="hasAnchor">
<a href="#too-many-values-per-categorical-feature" class="anchor"></a>Too many values per categorical feature</h3>
<p>When provided with categorical features, the logistic regression model fits a coefficient for all its values (except one which is taken as a reference). A common problem arises when there are too many values as each value will be taken by a small number of observations <span class="math inline">\(x_{i,j}\)</span> which makes the estimation of a logistic regression coefficient unstable:</p>
<div class="sourceCode" id="cb3"><html><body><pre class="r"><span class="no">x_disc_bad_idea</span> <span class="kw">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html">factor</a></span>(<span class="fu"><a href="https://rdrr.io/r/base/cut.html">cut</a></span>(<span class="no">x</span>,<span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span>(-<span class="fl">Inf</span>,<span class="fl">0.1</span>,<span class="fl">0.2</span>,<span class="fl">0.3</span>,<span class="fl">0.4</span>,<span class="fl">0.5</span>,<span class="fl">0.6</span>,<span class="fl">0.7</span>,<span class="fl">0.8</span>,<span class="fl">0.9</span>,+<span class="fl">Inf</span>)),<span class="kw">labels</span> <span class="kw">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span>(<span class="fl">1</span>,<span class="fl">2</span>,<span class="fl">3</span>,<span class="fl">4</span>,<span class="fl">5</span>,<span class="fl">6</span>,<span class="fl">7</span>,<span class="fl">8</span>,<span class="fl">9</span>,<span class="fl">10</span>))</pre></body></html></div>
<p>If we divide the training set in 10 and estimate the variance of each coefficient, we get:</p>
<p><img src="glmdisc_files/figure-html/unnamed-chunk-7-1.png" width="672" style="display: block; margin: auto;"></p>
<p>All intervals crossing <span class="math inline">\(0\)</span> are non-significant! We should group factor values to get a stable estimation and (hopefully) significant coefficient values.</p>
</div>
</div>
</div>
<div id="discretization-and-grouping-theoretical-background" class="section level1">
<h1 class="hasAnchor">
<a href="#discretization-and-grouping-theoretical-background" class="anchor"></a>Discretization and grouping: theoretical background</h1>
<div id="notations-1" class="section level2">
<h2 class="hasAnchor">
<a href="#notations-1" class="anchor"></a>Notations</h2>
<p>Let <span class="math inline">\(Q=(Q_j)_1^d\)</span> be the latent discretized transform of <span class="math inline">\(X\)</span>, i.e. taking values in <span class="math inline">\(\{0,\ldots,m_j\}\)</span> where the number of values of each covariate <span class="math inline">\(m_j\)</span> is also latent.</p>
<p>The fitted logistic regression model is now: <span class="math display">\[\ln \left( \frac{p_\theta(Y=1|q)}{p_\theta(Y=0|q)} \right) = \theta_0 + \sum_{j=1}^d \sum_{k=1}^{m_j} \theta_j^k*\mathbb{1}_{q_j=k}\]</span> Clearly, the number of parameters has grown which allows for flexible approximation of the true underlying model <span class="math inline">\(p(Y|Q)\)</span>.</p>
</div>
<div id="best-discretization" class="section level2">
<h2 class="hasAnchor">
<a href="#best-discretization" class="anchor"></a>Best discretization?</h2>
<p>Our goal is to obtain the model <span class="math inline">\(p_\theta(Y|e)\)</span> with best predictive power. As <span class="math inline">\(Q\)</span> and <span class="math inline">\(\theta\)</span> are both optimized, a formal goodness-of-fit criterion could be: <span class="math display">\[ (\hat{\theta},\hat{\bar{e}}) = \arg \max_{\theta,\bar{e}} \text{AIC}(p_\theta(\bar{y}|\bar{e})) \]</span> where AIC stands for Akaike Information Criterion.</p>
</div>
<div id="combinatorics" class="section level2">
<h2 class="hasAnchor">
<a href="#combinatorics" class="anchor"></a>Combinatorics</h2>
<p>The problem seems well-posed: if we were able to generate all discretization schemes transforming <span class="math inline">\(X\)</span> to <span class="math inline">\(Q\)</span>, learn <span class="math inline">\(p_\theta(y|e)\)</span> for each of them and compare their AIC values, the problem would be solved.</p>
<p>Unfortunately, there are way too many candidates to follow this procedure. Suppose we want to construct <span class="math inline">\(k\)</span> intervals of <span class="math inline">\(Q_j\)</span> given <span class="math inline">\(n\)</span> distinct <span class="math inline">\((x_{i,j})_1^n\)</span>. There is <span class="math inline">\(n \choose k\)</span> models. The true value of <span class="math inline">\(k\)</span> is unknown, so it must be looped over. Finally, as logistic regression is a multivariate model, the discretization of <span class="math inline">\(Q_j\)</span> can influence the discretization of <span class="math inline">\(Q^k\)</span>, <span class="math inline">\(k \neq j\)</span>.</p>
<p>As a consequence, existing approaches to discretization (in particular discretization of continuous attributes) rely on strong assumptions to simplify the search of good candidates as can be seen in the review of Ramírez‐Gallego, S. et al. (2016) - see References section.</p>
</div>
</div>
<div id="discretization-and-grouping-estimation" class="section level1">
<h1 class="hasAnchor">
<a href="#discretization-and-grouping-estimation" class="anchor"></a>Discretization and grouping: estimation</h1>
<div id="likelihood-estimation" class="section level2">
<h2 class="hasAnchor">
<a href="#likelihood-estimation" class="anchor"></a>Likelihood estimation</h2>
<p><span class="math inline">\(Q\)</span> can be introduced in <span class="math inline">\(p(Y|X)\)</span>: <span class="math display">\[\forall \: x,y, \; p(y|x) = \sum_e p(y|x,e)p(e|x)\]</span></p>
<p>First, we assume that all information about <span class="math inline">\(Y\)</span> in <span class="math inline">\(X\)</span> is already contained in <span class="math inline">\(Q\)</span> so that: <span class="math display">\[\forall \: x,y,q, \; p(y|x,q)=p(y|q)\]</span> Second, we assume the conditional independence of <span class="math inline">\(Q_j\)</span> given <span class="math inline">\(X_j\)</span>, i.e. knowing <span class="math inline">\(X_j\)</span>, the discretization <span class="math inline">\(Q_j\)</span> is independent of the other features <span class="math inline">\(X^k\)</span> and <span class="math inline">\(Q^k\)</span> for all <span class="math inline">\(k \neq j\)</span>: <span class="math display">\[\forall \:x, k\neq j, \; Q_j | x_j \perp Q_k | x_k\]</span> The first equation becomes: <span class="math display">\[\forall \: x,y, \; p(y|x) = \sum_q p(y|q) \prod_{j=1}^d p(q_j|x_j)\]</span> As said earlier, we consider only logistic regression models on discretized data <span class="math inline">\(p_\theta(y|e)\)</span>. Additionnally, it seems like we have to make further assumptions on the nature of the relationship of <span class="math inline">\(q_j\)</span> to <span class="math inline">\(x_j\)</span>. We chose to use polytomous logistic regressions for continuous <span class="math inline">\(X_j\)</span> and contengency tables for qualitative <span class="math inline">\(X_j\)</span>. This is an arbitrary choice and future versions will include the possibility of plugging your own model.</p>
<p>The first equation becomes: <span class="math display">\[\forall \: x,y, \; p(y|x) = \sum_q p_\theta(y|q) \prod_{j=1}^d p_{\alpha_j}(q_j|x_j)\]</span></p>
</div>
<div id="the-sem-algorithm" class="section level2">
<h2 class="hasAnchor">
<a href="#the-sem-algorithm" class="anchor"></a>The SEM algorithm</h2>
<p>It is still hard to optimize over <span class="math inline">\(p(y|x;\theta,\alpha)\)</span> as the number of candidate discretizations is gigantic as said earlier.</p>
<p>However, calculating <span class="math inline">\(p(y,q|x)\)</span> is easy: <span class="math display">\[\forall \: x,y, \; p(y,q|x) = p_\theta(y|q) \prod_{j=1}^d p_{\alpha_j}(q_j|x_j)\]</span></p>
<p>As a consequence, we will draw random candidates <span class="math inline">\(e\)</span> approximately at the mode of the distribution <span class="math inline">\(p(y,\cdot|x)\)</span> using an SEM algorithm (see References section).</p>
</div>
<div id="gibbs-sampling" class="section level2">
<h2 class="hasAnchor">
<a href="#gibbs-sampling" class="anchor"></a>Gibbs sampling</h2>
<p>To update, at each random draw, the parameters <span class="math inline">\(\theta\)</span> and <span class="math inline">\(\alpha\)</span> and propose a new discretization <span class="math inline">\(e\)</span>, we use the following equation: <span class="math display">\[p(q_j|x_j,y,q_{\{-j\}}) \propto p_\theta(y|q) p_{\alpha_j}(q_j|x_j)\]</span> Note that we draw <span class="math inline">\(q_j\)</span> knowing all other variables, especially <span class="math inline">\(q_{-\{j\}}\)</span>: this is called a Gibbs sampler (see References section). It is an MCMC method which target distribution is <span class="math inline">\(p(q|x,y)\)</span>.</p>
</div>
</div>
<div id="interaction-discovery" class="section level1">
<h1 class="hasAnchor">
<a href="#interaction-discovery" class="anchor"></a>Interaction discovery</h1>
<div id="what-do-interactions-mean" class="section level2">
<h2 class="hasAnchor">
<a href="#what-do-interactions-mean" class="anchor"></a>What do “interactions” mean?</h2>
<p>In a continuous setting, if there is an interaction between features <span class="math inline">\(k &gt; l\)</span>, then the logistic regression model becomes <span class="math inline">\(\text{logit}[p_\theta(1|x)] = \theta_0 + \sum_{j=1}^d \theta_j x_j + \theta_{kl} x^k x^l\)</span>. It amounts to adding some flexibility to the model by introducing a non-linearity, as was done with discretization.</p>
</div>
<div id="our-proposal" class="section level2">
<h2 class="hasAnchor">
<a href="#our-proposal" class="anchor"></a>Our proposal</h2>
<p>We consider that features interacting in the logistic regression model are unknown and denote by <span class="math inline">\(\Delta\)</span> the random matrix and <span class="math inline">\(\delta\)</span> its observation with <span class="math inline">\(\delta_{kl} = 0\)</span> for <span class="math inline">\(k \leq l\)</span>, <span class="math inline">\(\delta_{kl} = 1\)</span> for <span class="math inline">\(k &gt; l\)</span> if features <span class="math inline">\(k\)</span> and <span class="math inline">\(l\)</span> interact, <span class="math inline">\(\delta_{kl} = 0\)</span> otherwise.</p>
<p>Introducing <span class="math inline">\(\Delta\)</span> as a latent variable as we have done for discretization, we get <span class="math inline">\(p(\delta|x,y) \propto p_\theta(y|x,\delta) = \text{logit}^{-1} [\theta_0 + \sum_{j=1}^d \theta_j x_j + \sum_{k&gt;l} \delta_{kl} \theta_{kl} x^k x^l]\)</span>.</p>
<p>Again, a simple strategy would consist in estimating <span class="math inline">\(\hat{\theta}_\delta\)</span> for every value of <span class="math inline">\(\Delta\)</span> i.e. <span class="math inline">\(2^{\frac{d(d-1)}{2}}\)</span> models and compare their AIC or BIC values.</p>
<p>As a consequence of this untractable approach, we propose an MCMC procedure to draw <span class="math inline">\(\delta\)</span> from <span class="math inline">\(p(\delta|x,y)\)</span>.</p>
</div>
<div id="an-mcmc-estimation-procedure" class="section level2">
<h2 class="hasAnchor">
<a href="#an-mcmc-estimation-procedure" class="anchor"></a>An MCMC estimation procedure</h2>
<div id="the-metropolis-hastings-algorithm" class="section level3">
<h3 class="hasAnchor">
<a href="#the-metropolis-hastings-algorithm" class="anchor"></a>The Metropolis-Hastings algorithm</h3>
<p>The Metropolis-Hastings algorithm (see References section) is an MCMC procedure which relies on: 1. A proposal distribution <span class="math inline">\(q_{kl}\)</span>, the conditional probability of proposing to change the current state of <span class="math inline">\(\delta_{kl}\)</span>. 2. The acceptance distribution <span class="math inline">\(a_{kl}\)</span>, the conditional probability to accept changing the current state of <span class="math inline">\(\delta_{kl}\)</span>.</p>
<p>The transition probability is the product of these terms: <span class="math inline">\(p_{kl} = q_{kl}a_{kl}\)</span>.</p>
</div>
<div id="proposal" class="section level3">
<h3 class="hasAnchor">
<a href="#proposal" class="anchor"></a>Proposal</h3>
<p>We must choose a proposal distribution <span class="math inline">\(q_{kl}\)</span>. We suspect that a good hint for knowing if there effectively is an interaction between features <span class="math inline">\(k\)</span> and <span class="math inline">\(l\)</span> is to compare the bivariate logistic regression model with <span class="math inline">\(x_k\)</span> and <span class="math inline">\(x_l\)</span> to the logistic regression model containing <span class="math inline">\(x_k\)</span>, <span class="math inline">\(x_l\)</span> and their interaction term. Comparison can be done via the BIC criterion for example: <span class="math inline">\(\text{hint}_{kl} \propto \exp(-\text{BIC}(\text{with interaction}) + \text{BIC}(\text{without interaction}))\)</span>.</p>
<p>Given the current state of <span class="math inline">\(\Delta\)</span> denoted by <span class="math inline">\(\delta^{\text{current}}\)</span>, we want to propose changing the <span class="math inline">\(\delta_{kl}\)</span> entry if it is far from our “hint”, i.e.: <span class="math inline">\(q_{kl}=|\delta^{\text{current}}_{kl} - \text{hint}_{kl}|\)</span>.</p>
<p>We draw <span class="math inline">\(kl\)</span> from <span class="math inline">\(q_{kl}\)</span> and subsequently obtain <span class="math inline">\(\delta^{\text{new}}\)</span> which <span class="math inline">\(kl\)</span> entry is <span class="math inline">\(1-\delta^{\text{current}}_{kl}\)</span>.</p>
<p>This gives <span class="math inline">\(a_{kl} = min(1,\exp(-\text{BIC}(\delta^{\text{new}})+\text{BIC}(\delta^{\text{current}}))\frac{q_{kl}}{1-q_{kl}})\)</span>.</p>
<p>This approach would work as a “stand-alone” model selection tool but it can also be included in the discretization algorithm described in the previous section. That was developed in the <code>glmdisc</code> package.</p>
</div>
</div>
</div>
<div id="the-glmdisc-package" class="section level1">
<h1 class="hasAnchor">
<a href="#the-glmdisc-package" class="anchor"></a>The <code>glmdisc</code> package</h1>
<div id="the-glmdisc-function" class="section level2">
<h2 class="hasAnchor">
<a href="#the-glmdisc-function" class="anchor"></a>The <code>glmdisc</code> function</h2>
<p>The <code>glmdisc</code> function implements the algorithm discribed in the previous section. Its parameters are described first, then its internals are briefly discussed. We finally focus on its ouptuts.</p>
<div id="parameters" class="section level3">
<h3 class="hasAnchor">
<a href="#parameters" class="anchor"></a>Parameters</h3>
<p>The number of iterations in the SEM algorithm is controlled through the <code>iter</code> parameter. It can be useful to first run the <code>glmdisc</code> function with a low (10-50) <code>iter</code> parameter so you can have a better idea of how much time your code will run.</p>
<p>The <code>validation</code> and <code>test</code> boolean parameters control if the provided dataset should be divided into training, validation and/or test sets. The validation set aims at evaluating the quality of the model fit at each iteration while the test set provides the quality measure of the final chosen model.</p>
<p>The <code>criterion</code> parameters lets the user choose between standard model selection statistics like <code>aic</code> and <code>bic</code> and the <code>gini</code> index performance measure (proportional to the more traditional AUC measure). Note that if <code>validation=TRUE</code>, there is no need to penalize the log-likelihood and <code>aic</code> and <code>bic</code> become equivalent. On the contrary if <code>criterion="gini"</code> and <code>validation=FALSE</code> then the algorithm may overfit the training data.</p>
<p>The <code>m_start</code> parameter controls the maximum number of categories of <span class="math inline">\(Q_j\)</span> for <span class="math inline">\(X_j\)</span> continuous. The SEM algorithm will start with random <span class="math inline">\(Q_j\)</span> taking values in <span class="math inline">\(\{1,m_{\text{start}}\}\)</span>. For qualitative features <span class="math inline">\(X_j\)</span>, <span class="math inline">\(Q_j\)</span> is initialized with as many values as <span class="math inline">\(X_j\)</span> so that <code>m_start</code> has no effect.</p>
<p>The <code>reg_type</code> parameter controls the model used to fit <span class="math inline">\(Q_j\)</span> to continuous <span class="math inline">\(X_j\)</span>. The default behavior (<code>reg_type=poly</code>) uses <code>multinom</code> from the <code>nnet</code> package which is a polytomous logistic regression (see References section). The other <code>reg_type</code> value is <code>polr</code> from the <code>MASS</code> package which is an ordered logistic regression (simpler to estimate but less flexible as it fits way fewer parameters).</p>
<p>Empirical studies show that with a reasonably small training dataset (<span class="math inline">\(\leq 100 000\)</span> rows) and a small <code>m_start</code> parameter (<span class="math inline">\(\leq 20\)</span>), approximately <span class="math inline">\(500\)</span> to <span class="math inline">\(1500\)</span> iterations are largely sufficient to obtain a satisfactory model <span class="math inline">\(p_\theta(y|q)\)</span>.</p>
</div>
<div id="internals" class="section level3">
<h3 class="hasAnchor">
<a href="#internals" class="anchor"></a>Internals</h3>
<p>First, the discretized version <span class="math inline">\(Q\)</span> of <span class="math inline">\(X\)</span> is initialized at random using the user-provided maximum number of values for quantitative values through the <code>m_start</code> parameter.</p>
<p>Then we iterate <code>times</code>: First, the model <span class="math inline">\(p_\theta(y|q)\)</span>, where <span class="math inline">\(q\)</span> denotes the current value of <span class="math inline">\(Q\)</span>, is adjusted. Then, for each feature <span class="math inline">\(X_j\)</span>, the model <span class="math inline">\(p_{\alpha_j}(q|x)\)</span> is adjusted (depending on the type of <span class="math inline">\(X_j\)</span> it can be a polytomous logistic regression or a contengency table). Finally, we draw a new candidate <span class="math inline">\(q_j\)</span> with probability <span class="math inline">\(p_\theta(y|q)p_{\alpha_j}(q_j|x_j)\)</span>.</p>
<p>The performance metric chosen through the <code>criterion</code> parameter determines the best candidate <span class="math inline">\(q\)</span>.</p>
</div>
<div id="results" class="section level3">
<h3 class="hasAnchor">
<a href="#results" class="anchor"></a>Results</h3>
<p>First we simulate a “true” underlying discrete model:</p>
<div class="sourceCode" id="cb4"><html><body><pre class="r"><span class="no">x</span> <span class="kw">=</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span>(<span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span>(<span class="fl">300</span>), <span class="kw">nrow</span> <span class="kw">=</span> <span class="fl">100</span>, <span class="kw">ncol</span> <span class="kw">=</span> <span class="fl">3</span>)
<span class="no">cuts</span> <span class="kw">=</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span>(<span class="fl">0</span>,<span class="fl">1</span>,<span class="kw">length.out</span><span class="kw">=</span> <span class="fl">4</span>)
<span class="no">xd</span> <span class="kw">=</span> <span class="fu"><a href="https://rdrr.io/r/base/apply.html">apply</a></span>(<span class="no">x</span>,<span class="fl">2</span>, <span class="kw">function</span>(<span class="no">col</span>) <span class="fu"><a href="https://rdrr.io/r/base/numeric.html">as.numeric</a></span>(<span class="fu"><a href="https://rdrr.io/r/base/cut.html">cut</a></span>(<span class="no">col</span>,<span class="no">cuts</span>)))
<span class="no">theta</span> <span class="kw">=</span> <span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span>(<span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span>(<span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span>(<span class="fl">0</span>,<span class="fl">0</span>,<span class="fl">0</span>,<span class="fl">2</span>,<span class="fl">2</span>,<span class="fl">2</span>,-<span class="fl">2</span>,-<span class="fl">2</span>,-<span class="fl">2</span>),<span class="kw">ncol</span><span class="kw">=</span><span class="fl">3</span>,<span class="kw">nrow</span><span class="kw">=</span><span class="fl">3</span>))
<span class="no">log_odd</span> <span class="kw">=</span> <span class="fu"><a href="https://rdrr.io/r/base/colSums.html">rowSums</a></span>(<span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span>(<span class="fu"><a href="https://rdrr.io/r/base/lapply.html">sapply</a></span>(<span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq_along</a></span>(<span class="no">xd</span>[,<span class="fl">1</span>]), <span class="kw">function</span>(<span class="no">row_id</span>) <span class="fu"><a href="https://rdrr.io/r/base/lapply.html">sapply</a></span>(<span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq_along</a></span>(<span class="no">xd</span>[<span class="no">row_id</span>,]),
<span class="kw">function</span>(<span class="no">element</span>) <span class="no">theta</span>[<span class="no">xd</span>[<span class="no">row_id</span>,<span class="no">element</span>],<span class="no">element</span>]))))
<span class="no">y</span> <span class="kw">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">rbinom</a></span>(<span class="fl">100</span>,<span class="fl">1</span>,<span class="fl">1</span>/(<span class="fl">1</span>+<span class="fu"><a href="https://rdrr.io/r/base/Log.html">exp</a></span>(-<span class="no">log_odd</span>)))</pre></body></html></div>
<p>The <code>glmdisc</code> function will try to “recover” the hidden true discretization <code>xd</code> when provided only with <code>x</code> and <code>y</code>:</p>
<div class="sourceCode" id="cb5"><html><body><pre class="r"><span class="fu"><a href="https://rdrr.io/r/base/library.html">library</a></span>(<span class="no">glmdisc</span>)
<span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span>(<span class="fl">123</span>)
<span class="no">discretization</span> <span class="kw">&lt;-</span> <span class="fu"><a href="../reference/glmdisc.html">glmdisc</a></span>(<span class="no">x</span>,<span class="no">y</span>,<span class="kw">iter</span><span class="kw">=</span><span class="fl">50</span>,<span class="kw">m_start</span><span class="kw">=</span><span class="fl">5</span>,<span class="kw">test</span><span class="kw">=</span><span class="fl">FALSE</span>,<span class="kw">validation</span><span class="kw">=</span><span class="fl">FALSE</span>,<span class="kw">criterion</span><span class="kw">=</span><span class="st">"aic"</span>,<span class="kw">interact</span><span class="kw">=</span><span class="fl">FALSE</span>)</pre></body></html></div>
<p>For now, we simulated a discrete model without interactions. We can verify that our proposed approach for interaction discovery selects a model without interactions.</p>
<div class="sourceCode" id="cb6"><html><body><pre class="r"><span class="no">all_formula</span> <span class="kw">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span>()

<span class="kw">for</span> (<span class="no">l</span> <span class="kw">in</span> <span class="fl">1</span>:<span class="fl">10</span>) {
     <span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span>(<span class="no">l</span>)
     <span class="no">x</span> <span class="kw">=</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span>(<span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span>(<span class="fl">300</span>), <span class="kw">nrow</span> <span class="kw">=</span> <span class="fl">100</span>, <span class="kw">ncol</span> <span class="kw">=</span> <span class="fl">3</span>)
     <span class="no">cuts</span> <span class="kw">=</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span>(<span class="fl">0</span>,<span class="fl">1</span>,<span class="kw">length.out</span><span class="kw">=</span> <span class="fl">4</span>)
     <span class="no">xd</span> <span class="kw">=</span> <span class="fu"><a href="https://rdrr.io/r/base/apply.html">apply</a></span>(<span class="no">x</span>,<span class="fl">2</span>, <span class="kw">function</span>(<span class="no">col</span>) <span class="fu"><a href="https://rdrr.io/r/base/numeric.html">as.numeric</a></span>(<span class="fu"><a href="https://rdrr.io/r/base/cut.html">cut</a></span>(<span class="no">col</span>,<span class="no">cuts</span>)))
     <span class="no">theta</span> <span class="kw">=</span> <span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span>(<span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span>(<span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span>(<span class="fl">0</span>,<span class="fl">0</span>,<span class="fl">0</span>,<span class="fl">2</span>,<span class="fl">2</span>,<span class="fl">2</span>,-<span class="fl">2</span>,-<span class="fl">2</span>,-<span class="fl">2</span>),<span class="kw">ncol</span><span class="kw">=</span><span class="fl">3</span>,<span class="kw">nrow</span><span class="kw">=</span><span class="fl">3</span>))
     <span class="no">log_odd</span> <span class="kw">=</span> <span class="fu"><a href="https://rdrr.io/r/base/colSums.html">rowSums</a></span>(<span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span>(<span class="fu"><a href="https://rdrr.io/r/base/lapply.html">sapply</a></span>(<span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq_along</a></span>(<span class="no">xd</span>[,<span class="fl">1</span>]), <span class="kw">function</span>(<span class="no">row_id</span>) <span class="fu"><a href="https://rdrr.io/r/base/lapply.html">sapply</a></span>(<span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq_along</a></span>(<span class="no">xd</span>[<span class="no">row_id</span>,]),
     <span class="kw">function</span>(<span class="no">element</span>) <span class="no">theta</span>[<span class="no">xd</span>[<span class="no">row_id</span>,<span class="no">element</span>],<span class="no">element</span>]))))
     <span class="no">y</span> <span class="kw">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">rbinom</a></span>(<span class="fl">100</span>,<span class="fl">1</span>,<span class="fl">1</span>/(<span class="fl">1</span>+<span class="fu"><a href="https://rdrr.io/r/base/Log.html">exp</a></span>(-<span class="no">log_odd</span>)))

     <span class="no">discretization</span> <span class="kw">&lt;-</span> <span class="fu"><a href="../reference/glmdisc.html">glmdisc</a></span>(<span class="no">x</span>,<span class="no">y</span>,<span class="kw">iter</span><span class="kw">=</span><span class="fl">50</span>,<span class="kw">m_start</span><span class="kw">=</span><span class="fl">5</span>,<span class="kw">test</span><span class="kw">=</span><span class="fl">FALSE</span>,<span class="kw">validation</span><span class="kw">=</span><span class="fl">FALSE</span>,<span class="kw">criterion</span><span class="kw">=</span><span class="st">"aic"</span>,<span class="kw">interact</span><span class="kw">=</span><span class="fl">TRUE</span>)
     <span class="no">all_formula</span><span class="kw">[[</span><span class="no">l</span>]] <span class="kw">&lt;-</span> <span class="no">discretization</span>@<span class="kw">best.disc</span>$<span class="no">formulaOfBbestestLogisticRegression</span>
}

<span class="co">#barplot(table(grepl(":",all_formula)),names.arg=c("No interaction","One interaction"),xlim = 2)</span></pre></body></html></div>
<p>Conversely, we can simulate a discrete model with an interaction and report how many times the proposed algorithm detected the right interaction.</p>
<div class="sourceCode" id="cb7"><html><body><pre class="r">
<span class="no">all_formula</span> <span class="kw">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span>()

<span class="kw">for</span> (<span class="no">l</span> <span class="kw">in</span> <span class="fl">1</span>:<span class="fl">10</span>) {
     <span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span>(<span class="fl">100</span>+<span class="no">l</span>)
     <span class="no">x</span> <span class="kw">=</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span>(<span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span>(<span class="fl">3000</span>), <span class="kw">nrow</span> <span class="kw">=</span> <span class="fl">1000</span>, <span class="kw">ncol</span> <span class="kw">=</span> <span class="fl">3</span>)
     <span class="no">cuts</span> <span class="kw">=</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span>(<span class="fl">0</span>,<span class="fl">1</span>,<span class="kw">length.out</span><span class="kw">=</span> <span class="fl">4</span>)
     <span class="no">xd</span> <span class="kw">=</span> <span class="fu"><a href="https://rdrr.io/r/base/apply.html">apply</a></span>(<span class="no">x</span>,<span class="fl">2</span>, <span class="kw">function</span>(<span class="no">col</span>) <span class="fu"><a href="https://rdrr.io/r/base/numeric.html">as.numeric</a></span>(<span class="fu"><a href="https://rdrr.io/r/base/cut.html">cut</a></span>(<span class="no">col</span>,<span class="no">cuts</span>)))
     <span class="no">theta</span> <span class="kw">=</span> <span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span>(<span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span>(<span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span>(<span class="fl">0</span>,<span class="fl">0</span>,<span class="fl">0</span>,<span class="fl">2</span>,<span class="fl">2</span>,<span class="fl">2</span>,-<span class="fl">2</span>,-<span class="fl">2</span>,-<span class="fl">2</span>),<span class="kw">ncol</span><span class="kw">=</span><span class="fl">3</span>,<span class="kw">nrow</span><span class="kw">=</span><span class="fl">3</span>))
     <span class="no">log_odd</span> <span class="kw">=</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span>(<span class="fl">0</span>,<span class="fl">1000</span>,<span class="fl">1</span>)
       <span class="kw">for</span> (<span class="no">i</span> <span class="kw">in</span> <span class="fl">1</span>:<span class="fl">1000</span>) {
          <span class="no">log_odd</span>[<span class="no">i</span>] <span class="kw">=</span> <span class="fl">2</span>*(<span class="no">xd</span>[<span class="no">i</span>,<span class="fl">1</span>]<span class="kw">==</span><span class="fl">1</span>)+
               (-<span class="fl">2</span>)*(<span class="no">xd</span>[<span class="no">i</span>,<span class="fl">1</span>]<span class="kw">==</span><span class="fl">2</span>)+
               <span class="fl">2</span>*(<span class="no">xd</span>[<span class="no">i</span>,<span class="fl">1</span>]<span class="kw">==</span><span class="fl">1</span>)*(<span class="no">xd</span>[<span class="no">i</span>,<span class="fl">2</span>]<span class="kw">==</span><span class="fl">1</span>)+
               <span class="fl">4</span>*(<span class="no">xd</span>[<span class="no">i</span>,<span class="fl">1</span>]<span class="kw">==</span><span class="fl">2</span>)*(<span class="no">xd</span>[<span class="no">i</span>,<span class="fl">2</span>]<span class="kw">==</span><span class="fl">2</span>)+
               (-<span class="fl">2</span>)*(<span class="no">xd</span>[<span class="no">i</span>,<span class="fl">2</span>]<span class="kw">==</span><span class="fl">1</span>)*(<span class="no">xd</span>[<span class="no">i</span>,<span class="fl">3</span>]<span class="kw">==</span><span class="fl">2</span>)+
               (-<span class="fl">4</span>)*(<span class="no">xd</span>[<span class="no">i</span>,<span class="fl">2</span>]<span class="kw">==</span><span class="fl">2</span>)*(<span class="no">xd</span>[<span class="no">i</span>,<span class="fl">3</span>]<span class="kw">==</span><span class="fl">1</span>)+
               <span class="fl">1</span>*(<span class="no">xd</span>[<span class="no">i</span>,<span class="fl">2</span>]<span class="kw">==</span><span class="fl">3</span>)*(<span class="no">xd</span>[<span class="no">i</span>,<span class="fl">3</span>]<span class="kw">==</span><span class="fl">1</span>)+
               (-<span class="fl">1</span>)*(<span class="no">xd</span>[<span class="no">i</span>,<span class="fl">1</span>]<span class="kw">==</span><span class="fl">1</span>)*(<span class="no">xd</span>[<span class="no">i</span>,<span class="fl">3</span>]<span class="kw">==</span><span class="fl">3</span>)+
               <span class="fl">3</span>*(<span class="no">xd</span>[<span class="no">i</span>,<span class="fl">1</span>]<span class="kw">==</span><span class="fl">3</span>)*(<span class="no">xd</span>[<span class="no">i</span>,<span class="fl">3</span>]<span class="kw">==</span><span class="fl">2</span>)+
               (-<span class="fl">1</span>)*(<span class="no">xd</span>[<span class="no">i</span>,<span class="fl">1</span>]<span class="kw">==</span><span class="fl">2</span>)*(<span class="no">xd</span>[<span class="no">i</span>,<span class="fl">3</span>]<span class="kw">==</span><span class="fl">3</span>)
       }

     <span class="no">y</span> <span class="kw">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">rbinom</a></span>(<span class="fl">1000</span>,<span class="fl">1</span>,<span class="fl">1</span>/(<span class="fl">1</span>+<span class="fu"><a href="https://rdrr.io/r/base/Log.html">exp</a></span>(-<span class="no">log_odd</span>)))

     <span class="no">discretization</span> <span class="kw">&lt;-</span> <span class="fu"><a href="../reference/glmdisc.html">glmdisc</a></span>(<span class="no">x</span>,<span class="no">y</span>,<span class="kw">iter</span><span class="kw">=</span><span class="fl">50</span>,<span class="kw">m_start</span><span class="kw">=</span><span class="fl">5</span>,<span class="kw">test</span><span class="kw">=</span><span class="fl">FALSE</span>,<span class="kw">validation</span><span class="kw">=</span><span class="fl">FALSE</span>,<span class="kw">criterion</span><span class="kw">=</span><span class="st">"aic"</span>,<span class="kw">interact</span><span class="kw">=</span><span class="fl">TRUE</span>)
     <span class="no">all_formula</span><span class="kw">[[</span><span class="no">l</span>]] <span class="kw">&lt;-</span> <span class="no">discretization</span>@<span class="kw">best.disc</span>$<span class="no">formulaOfBbestestLogisticRegression</span>
}

<span class="co">#barplot(table(grepl(":",all_formula)),names.arg=c("No interaction","One interaction"),xlim = 2)</span></pre></body></html></div>
</div>
<div id="the-parameters-slot" class="section level3">
<h3 class="hasAnchor">
<a href="#the-parameters-slot" class="anchor"></a>The <code>parameters</code> slot</h3>
<p>The <code>parameters</code> slot refers back to the user-provided parameters given to the <code>glmdisc</code> function. Consequently, users can compare results of different <code>glmdisc</code> with respect to their parameters.</p>
<div class="sourceCode" id="cb8"><html><body><pre class="r"><span class="no">discretization</span>@<span class="kw">parameters</span>
<span class="co">#&gt; $test</span>
<span class="co">#&gt; [1] FALSE</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; $validation</span>
<span class="co">#&gt; [1] FALSE</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; $criterion</span>
<span class="co">#&gt; [1] "aic"</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; $iter</span>
<span class="co">#&gt; [1] 50</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; $m_start</span>
<span class="co">#&gt; [1] 5</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; $reg_type</span>
<span class="co">#&gt; [1] "poly"</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; $types_data</span>
<span class="co">#&gt; [1] "numeric" "numeric" "numeric"</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; $encoder</span>
<span class="co">#&gt; Dummy Variable Object</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Formula: ~V1 + V2 + V3</span>
<span class="co">#&gt; &lt;environment: 0x7faf4d16f8f0&gt;</span>
<span class="co">#&gt; 3 variables, 3 factors</span>
<span class="co">#&gt; Variables and levels will be separated by '_'</span>
<span class="co">#&gt; A less than full rank encoding is used</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; $interact</span>
<span class="co">#&gt; [1] TRUE</span></pre></body></html></div>
</div>
<div id="the-best-disc-slot" class="section level3">
<h3 class="hasAnchor">
<a href="#the-best-disc-slot" class="anchor"></a>The <code>best.disc</code> slot</h3>
<p>The <code>best.disc</code> slot contains all that is needed to reproduce the best discretization scheme obtained by <code>glmdisc</code> function: the first element is the best logistic regression model obtained <span class="math inline">\(p_\theta(y|e)\)</span>; the second element is its associated “link” function, i.e. either the <code>multinom</code> or <code>polr</code> functions fit to each continuous features and the contengency tables fit to each quantitative features.</p>
<div class="sourceCode" id="cb9"><html><body><pre class="r"><span class="no">discretization</span>@<span class="kw">best.disc</span><span class="kw">[[</span><span class="fl">1</span>]]
<span class="co">#&gt; $coefficients</span>
<span class="co">#&gt; [1] -1.32943135  1.35760660 -0.93817671  0.69734447 -0.30888302  0.09622603</span>
<span class="co">#&gt; [7]  0.18587674 -0.47209789  0.07356415</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; $fitted.values</span>
<span class="co">#&gt;    [1] 0.1729661 0.7746227 0.7269985 0.7269985 0.6134946 0.8374954 0.7041491</span>
<span class="co">#&gt;    [8] 0.6134946 0.1896267 0.1729661 0.4791031 0.8216251 0.2570834 0.7269985</span>
<span class="co">#&gt;   [15] 0.6134946 0.7746227 0.7746227 0.3168282 0.1896267 0.2362237 0.7269985</span>
<span class="co">#&gt;   [22] 0.1729661 0.6134946 0.7544128 0.6397636 0.7746227 0.7746227 0.1729661</span>
<span class="co">#&gt;   [29] 0.6397636 0.6134946 0.1729661 0.7746227 0.8374954 0.7544128 0.7041491</span>
<span class="co">#&gt;   [36] 0.2118102 0.6397636 0.6397636 0.7274523 0.6134946 0.1896267 0.1896267</span>
<span class="co">#&gt;   [43] 0.7746227 0.6134946 0.6397636 0.6397636 0.8216251 0.3416221 0.6397636</span>
<span class="co">#&gt;   [50] 0.2570834 0.6397636 0.1896267 0.7544128 0.6397636 0.7746227 0.7746227</span>
<span class="co">#&gt;   [57] 0.2570834 0.6397636 0.2362237 0.7746227 0.1896267 0.6134946 0.1896267</span>
<span class="co">#&gt;   [64] 0.2597384 0.3416221 0.1519797 0.8374954 0.7544128 0.8374954 0.2570834</span>
<span class="co">#&gt;   [71] 0.1729661 0.6397636 0.7269985 0.2570834 0.1896267 0.2570834 0.8374954</span>
<span class="co">#&gt;   [78] 0.2597384 0.7746227 0.1896267 0.7544128 0.2362237 0.7269985 0.1729661</span>
<span class="co">#&gt;   [85] 0.6397636 0.7746227 0.7746227 0.2570834 0.7746227 0.2570834 0.2570834</span>
<span class="co">#&gt;   [92] 0.7544128 0.1729661 0.1896267 0.8374954 0.7746227 0.7269985 0.7544128</span>
<span class="co">#&gt;   [99] 0.2570834 0.8374954 0.2570834 0.7746227 0.7269985 0.6134946 0.7746227</span>
<span class="co">#&gt;  [106] 0.3416221 0.2570834 0.7746227 0.2570834 0.2597384 0.7746227 0.6134946</span>
<span class="co">#&gt;  [113] 0.2570834 0.7746227 0.6397636 0.7269985 0.6134946 0.7746227 0.7746227</span>
<span class="co">#&gt;  [120] 0.2597384 0.7746227 0.7746227 0.6134946 0.1896267 0.7544128 0.6134946</span>
<span class="co">#&gt;  [127] 0.7544128 0.7041491 0.1729661 0.2570834 0.7746227 0.1896267 0.7746227</span>
<span class="co">#&gt;  [134] 0.8216251 0.2362237 0.7746227 0.2570834 0.8374954 0.7746227 0.2570834</span>
<span class="co">#&gt;  [141] 0.7746227 0.6134946 0.7746227 0.1080889 0.2570834 0.7746227 0.1080889</span>
<span class="co">#&gt;  [148] 0.1729661 0.1729661 0.2570834 0.7544128 0.8216251 0.7746227 0.6134946</span>
<span class="co">#&gt;  [155] 0.2570834 0.7746227 0.1896267 0.6134946 0.6134946 0.6134946 0.7544128</span>
<span class="co">#&gt;  [162] 0.3416221 0.1729661 0.6397636 0.2570834 0.8216251 0.2570834 0.6397636</span>
<span class="co">#&gt;  [169] 0.6397636 0.6397636 0.6134946 0.6134946 0.6397636 0.7746227 0.7269985</span>
<span class="co">#&gt;  [176] 0.6134946 0.8216251 0.6134946 0.1896267 0.2597384 0.1896267 0.6397636</span>
<span class="co">#&gt;  [183] 0.2570834 0.1080889 0.8374954 0.8374954 0.1729661 0.8374954 0.1896267</span>
<span class="co">#&gt;  [190] 0.6397636 0.2362237 0.3168282 0.7746227 0.7746227 0.1896267 0.3416221</span>
<span class="co">#&gt;  [197] 0.7544128 0.6397636 0.2570834 0.7746227 0.8374954 0.8374954 0.7746227</span>
<span class="co">#&gt;  [204] 0.8216251 0.6397636 0.6397636 0.8216251 0.2362237 0.7544128 0.2570834</span>
<span class="co">#&gt;  [211] 0.6134946 0.6397636 0.2570834 0.6397636 0.7746227 0.2570834 0.2570834</span>
<span class="co">#&gt;  [218] 0.7544128 0.2387325 0.7544128 0.1896267 0.7746227 0.2387325 0.1896267</span>
<span class="co">#&gt;  [225] 0.3416221 0.1896267 0.2362237 0.6134946 0.7746227 0.7746227 0.2570834</span>
<span class="co">#&gt;  [232] 0.7544128 0.1519797 0.8374954 0.8374954 0.2387325 0.6397636 0.7746227</span>
<span class="co">#&gt;  [239] 0.6397636 0.6134946 0.7544128 0.6397636 0.2362237 0.2570834 0.6134946</span>
<span class="co">#&gt;  [246] 0.7746227 0.1896267 0.4791031 0.6397636 0.7746227 0.6397636 0.7746227</span>
<span class="co">#&gt;  [253] 0.7746227 0.7269985 0.6397636 0.7544128 0.7746227 0.8374954 0.7544128</span>
<span class="co">#&gt;  [260] 0.6397636 0.1729661 0.7544128 0.6397636 0.1896267 0.7544128 0.2570834</span>
<span class="co">#&gt;  [267] 0.4791031 0.7746227 0.6397636 0.2597384 0.7746227 0.7746227 0.8216251</span>
<span class="co">#&gt;  [274] 0.7746227 0.7544128 0.2570834 0.2362237 0.8374954 0.6134946 0.6134946</span>
<span class="co">#&gt;  [281] 0.6397636 0.2570834 0.2597384 0.6397636 0.7544128 0.7746227 0.7544128</span>
<span class="co">#&gt;  [288] 0.3168282 0.6397636 0.6134946 0.4791031 0.6397636 0.7746227 0.6134946</span>
<span class="co">#&gt;  [295] 0.7746227 0.2570834 0.6134946 0.7746227 0.8374954 0.1729661 0.7746227</span>
<span class="co">#&gt;  [302] 0.7544128 0.6397636 0.1896267 0.7746227 0.7269985 0.7746227 0.8374954</span>
<span class="co">#&gt;  [309] 0.7746227 0.2362237 0.2570834 0.2570834 0.2570834 0.6134946 0.7746227</span>
<span class="co">#&gt;  [316] 0.6134946 0.7269985 0.6134946 0.8216251 0.6397636 0.7544128 0.7746227</span>
<span class="co">#&gt;  [323] 0.8374954 0.6134946 0.6397636 0.2118102 0.8374954 0.6397636 0.2570834</span>
<span class="co">#&gt;  [330] 0.3168282 0.3416221 0.7544128 0.3416221 0.7746227 0.2597384 0.7544128</span>
<span class="co">#&gt;  [337] 0.7544128 0.6134946 0.6397636 0.1729661 0.7544128 0.7746227 0.2597384</span>
<span class="co">#&gt;  [344] 0.2597384 0.7746227 0.2362237 0.7041491 0.7746227 0.2118102 0.2362237</span>
<span class="co">#&gt;  [351] 0.6397636 0.6134946 0.8374954 0.6134946 0.7746227 0.7746227 0.1896267</span>
<span class="co">#&gt;  [358] 0.6134946 0.1896267 0.8374954 0.8216251 0.1896267 0.6134946 0.7544128</span>
<span class="co">#&gt;  [365] 0.6134946 0.2362237 0.8374954 0.1896267 0.2570834 0.1896267 0.3416221</span>
<span class="co">#&gt;  [372] 0.6134946 0.2570834 0.6397636 0.8216251 0.2570834 0.7544128 0.7269985</span>
<span class="co">#&gt;  [379] 0.7544128 0.7269985 0.2570834 0.6397636 0.2570834 0.7544128 0.1729661</span>
<span class="co">#&gt;  [386] 0.6134946 0.1729661 0.7746227 0.1896267 0.7746227 0.1080889 0.6397636</span>
<span class="co">#&gt;  [393] 0.7746227 0.7269985 0.6134946 0.2570834 0.7544128 0.7269985 0.1896267</span>
<span class="co">#&gt;  [400] 0.7269985 0.1896267 0.8374954 0.7746227 0.7269985 0.2570834 0.3416221</span>
<span class="co">#&gt;  [407] 0.7544128 0.7746227 0.2570834 0.4791031 0.6134946 0.1729661 0.1896267</span>
<span class="co">#&gt;  [414] 0.6397636 0.6134946 0.2570834 0.8216251 0.7746227 0.7746227 0.3416221</span>
<span class="co">#&gt;  [421] 0.8216251 0.1896267 0.2362237 0.7544128 0.6134946 0.7544128 0.1729661</span>
<span class="co">#&gt;  [428] 0.3416221 0.6397636 0.1896267 0.6134946 0.2570834 0.2362237 0.3168282</span>
<span class="co">#&gt;  [435] 0.6397636 0.2570834 0.7746227 0.2570834 0.6397636 0.7269985 0.7746227</span>
<span class="co">#&gt;  [442] 0.6134946 0.7544128 0.2570834 0.2570834 0.7746227 0.7746227 0.6397636</span>
<span class="co">#&gt;  [449] 0.7746227 0.2362237 0.6397636 0.7746227 0.1729661 0.6134946 0.7746227</span>
<span class="co">#&gt;  [456] 0.6134946 0.6397636 0.6134946 0.7544128 0.7544128 0.7746227 0.1896267</span>
<span class="co">#&gt;  [463] 0.7746227 0.2570834 0.1729661 0.3168282 0.2570834 0.6134946 0.7746227</span>
<span class="co">#&gt;  [470] 0.7746227 0.8374954 0.3168282 0.2362237 0.6397636 0.2570834 0.6397636</span>
<span class="co">#&gt;  [477] 0.8374954 0.6397636 0.6134946 0.2597384 0.3416221 0.1896267 0.8374954</span>
<span class="co">#&gt;  [484] 0.8374954 0.6134946 0.8216251 0.2570834 0.8374954 0.7746227 0.6134946</span>
<span class="co">#&gt;  [491] 0.1896267 0.6397636 0.6402906 0.3416221 0.7746227 0.6397636 0.1896267</span>
<span class="co">#&gt;  [498] 0.7544128 0.7746227 0.2570834 0.6134946 0.8374954 0.3416221 0.1896267</span>
<span class="co">#&gt;  [505] 0.2570834 0.2362237 0.6397636 0.7544128 0.6397636 0.7746227 0.7544128</span>
<span class="co">#&gt;  [512] 0.2570834 0.1729661 0.6134946 0.6397636 0.7544128 0.6397636 0.7746227</span>
<span class="co">#&gt;  [519] 0.6397636 0.8216251 0.6134946 0.7746227 0.6134946 0.6397636 0.4791031</span>
<span class="co">#&gt;  [526] 0.2362237 0.6397636 0.6134946 0.2362237 0.7746227 0.7746227 0.8216251</span>
<span class="co">#&gt;  [533] 0.7746227 0.2570834 0.8374954 0.6134946 0.6134946 0.7041491 0.7746227</span>
<span class="co">#&gt;  [540] 0.2362237 0.7269985 0.7746227 0.7746227 0.7746227 0.2570834 0.2570834</span>
<span class="co">#&gt;  [547] 0.3168282 0.6397636 0.7746227 0.8374954 0.7269985 0.3168282 0.7746227</span>
<span class="co">#&gt;  [554] 0.7269985 0.2570834 0.6397636 0.7544128 0.2570834 0.6397636 0.3168282</span>
<span class="co">#&gt;  [561] 0.6397636 0.6397636 0.7746227 0.2597384 0.6397636 0.6397636 0.7746227</span>
<span class="co">#&gt;  [568] 0.8216251 0.8374954 0.3416221 0.4791031 0.2597384 0.6397636 0.1896267</span>
<span class="co">#&gt;  [575] 0.2362237 0.1729661 0.2570834 0.6397636 0.7746227 0.6397636 0.2570834</span>
<span class="co">#&gt;  [582] 0.7544128 0.7269985 0.2597384 0.6397636 0.6397636 0.8216251 0.8374954</span>
<span class="co">#&gt;  [589] 0.7746227 0.6397636 0.7544128 0.7269985 0.6397636 0.2597384 0.6134946</span>
<span class="co">#&gt;  [596] 0.7269985 0.7544128 0.7746227 0.2570834 0.2570834 0.6397636 0.7269985</span>
<span class="co">#&gt;  [603] 0.7544128 0.1896267 0.2362237 0.8216251 0.7269985 0.7746227 0.7746227</span>
<span class="co">#&gt;  [610] 0.7041491 0.3416221 0.6397636 0.2570834 0.6397636 0.7544128 0.6397636</span>
<span class="co">#&gt;  [617] 0.7544128 0.6134946 0.6134946 0.6134946 0.7544128 0.6134946 0.2387325</span>
<span class="co">#&gt;  [624] 0.7746227 0.3416221 0.7746227 0.2362237 0.1896267 0.7746227 0.7746227</span>
<span class="co">#&gt;  [631] 0.7746227 0.6397636 0.4791031 0.2570834 0.2570834 0.7041491 0.1729661</span>
<span class="co">#&gt;  [638] 0.6134946 0.7746227 0.6397636 0.7746227 0.7544128 0.6397636 0.7544128</span>
<span class="co">#&gt;  [645] 0.2362237 0.7746227 0.2570834 0.4791031 0.8374954 0.2597384 0.2570834</span>
<span class="co">#&gt;  [652] 0.2570834 0.6397636 0.7544128 0.3416221 0.7544128 0.6397636 0.7746227</span>
<span class="co">#&gt;  [659] 0.7746227 0.2570834 0.2570834 0.8374954 0.2570834 0.2597384 0.2362237</span>
<span class="co">#&gt;  [666] 0.6134946 0.6402906 0.6397636 0.3168282 0.7544128 0.2362237 0.7544128</span>
<span class="co">#&gt;  [673] 0.2362237 0.7041491 0.6397636 0.6397636 0.7746227 0.6397636 0.7746227</span>
<span class="co">#&gt;  [680] 0.6397636 0.6134946 0.3416221 0.7544128 0.8374954 0.8216251 0.7746227</span>
<span class="co">#&gt;  [687] 0.6397636 0.6397636 0.6397636 0.7544128 0.2362237 0.6397636 0.7746227</span>
<span class="co">#&gt;  [694] 0.7746227 0.7041491 0.6134946 0.6134946 0.7269985 0.3168282 0.1896267</span>
<span class="co">#&gt;  [701] 0.2570834 0.7269985 0.7746227 0.7544128 0.2362237 0.7746227 0.7746227</span>
<span class="co">#&gt;  [708] 0.7544128 0.6397636 0.1519797 0.6397636 0.7544128 0.8216251 0.1729661</span>
<span class="co">#&gt;  [715] 0.2570834 0.7269985 0.8374954 0.6397636 0.6397636 0.6397636 0.6134946</span>
<span class="co">#&gt;  [722] 0.7746227 0.2570834 0.7746227 0.2570834 0.7269985 0.6397636 0.6134946</span>
<span class="co">#&gt;  [729] 0.7544128 0.6397636 0.2570834 0.8374954 0.2597384 0.2570834 0.6134946</span>
<span class="co">#&gt;  [736] 0.6134946 0.6134946 0.7544128 0.3168282 0.2570834 0.3168282 0.6397636</span>
<span class="co">#&gt;  [743] 0.2570834 0.7746227 0.3168282 0.7746227 0.1729661 0.7746227 0.7746227</span>
<span class="co">#&gt;  [750] 0.7269985 0.1896267 0.7746227 0.7746227 0.2597384 0.7746227 0.1896267</span>
<span class="co">#&gt;  [757] 0.7544128 0.6402906 0.7746227 0.1896267 0.2570834 0.6397636 0.2570834</span>
<span class="co">#&gt;  [764] 0.7746227 0.7269985 0.7269985 0.3416221 0.6134946 0.1896267 0.6397636</span>
<span class="co">#&gt;  [771] 0.7746227 0.6397636 0.2570834 0.6397636 0.3168282 0.7544128 0.1896267</span>
<span class="co">#&gt;  [778] 0.3416221 0.2570834 0.7746227 0.2570834 0.7746227 0.3416221 0.6397636</span>
<span class="co">#&gt;  [785] 0.7746227 0.6397636 0.2597384 0.4791031 0.7746227 0.6397636 0.6134946</span>
<span class="co">#&gt;  [792] 0.1729661 0.7746227 0.7544128 0.7746227 0.6397636 0.7746227 0.6397636</span>
<span class="co">#&gt;  [799] 0.7746227 0.6397636 0.7746227 0.2570834 0.1896267 0.6134946 0.7746227</span>
<span class="co">#&gt;  [806] 0.6397636 0.1519797 0.2570834 0.1080889 0.7746227 0.2570834 0.2570834</span>
<span class="co">#&gt;  [813] 0.2570834 0.7269985 0.1896267 0.3416221 0.7269985 0.2570834 0.6402906</span>
<span class="co">#&gt;  [820] 0.7274523 0.1729661 0.7746227 0.6397636 0.7746227 0.6134946 0.8374954</span>
<span class="co">#&gt;  [827] 0.6397636 0.2570834 0.7746227 0.3168282 0.2570834 0.7544128 0.7746227</span>
<span class="co">#&gt;  [834] 0.3416221 0.6397636 0.2570834 0.2387325 0.6397636 0.7746227 0.6397636</span>
<span class="co">#&gt;  [841] 0.4791031 0.7746227 0.6397636 0.2362237 0.6134946 0.6397636 0.1896267</span>
<span class="co">#&gt;  [848] 0.8374954 0.7746227 0.1729661 0.6397636 0.7746227 0.6134946 0.7269985</span>
<span class="co">#&gt;  [855] 0.7746227 0.7269985 0.7746227 0.1080889 0.7544128 0.1729661 0.7544128</span>
<span class="co">#&gt;  [862] 0.8374954 0.7746227 0.7746227 0.6397636 0.7746227 0.8374954 0.7269985</span>
<span class="co">#&gt;  [869] 0.3416221 0.7746227 0.1896267 0.7746227 0.6134946 0.3168282 0.6134946</span>
<span class="co">#&gt;  [876] 0.7746227 0.2570834 0.6397636 0.7269985 0.2387325 0.1896267 0.2570834</span>
<span class="co">#&gt;  [883] 0.1519797 0.1519797 0.6397636 0.6134946 0.2362237 0.8374954 0.2570834</span>
<span class="co">#&gt;  [890] 0.7746227 0.7746227 0.2362237 0.2362237 0.8216251 0.3416221 0.6134946</span>
<span class="co">#&gt;  [897] 0.6134946 0.2362237 0.6397636 0.7041491 0.2570834 0.1729661 0.1896267</span>
<span class="co">#&gt;  [904] 0.3168282 0.6134946 0.2597384 0.7746227 0.7746227 0.7746227 0.6397636</span>
<span class="co">#&gt;  [911] 0.7746227 0.1896267 0.2362237 0.2597384 0.7746227 0.1896267 0.6397636</span>
<span class="co">#&gt;  [918] 0.7746227 0.2570834 0.6397636 0.7746227 0.7746227 0.2362237 0.6397636</span>
<span class="co">#&gt;  [925] 0.2570834 0.8374954 0.2362237 0.7746227 0.2570834 0.6134946 0.7746227</span>
<span class="co">#&gt;  [932] 0.7544128 0.1896267 0.6134946 0.7746227 0.6397636 0.1519797 0.7544128</span>
<span class="co">#&gt;  [939] 0.8374954 0.3416221 0.2570834 0.6402906 0.7544128 0.8374954 0.7746227</span>
<span class="co">#&gt;  [946] 0.7544128 0.2362237 0.2362237 0.6134946 0.6397636 0.6397636 0.6134946</span>
<span class="co">#&gt;  [953] 0.6397636 0.7544128 0.1896267 0.7746227 0.8374954 0.8374954 0.7544128</span>
<span class="co">#&gt;  [960] 0.2570834 0.2570834 0.7274523 0.2387325 0.7544128 0.2597384 0.7746227</span>
<span class="co">#&gt;  [967] 0.2597384 0.1896267 0.7746227 0.7269985 0.6134946 0.2570834 0.6397636</span>
<span class="co">#&gt;  [974] 0.7746227 0.6397636 0.7269985 0.1896267 0.1896267 0.6397636 0.8374954</span>
<span class="co">#&gt;  [981] 0.7041491 0.7269985 0.1729661 0.8374954 0.2570834 0.6397636 0.7746227</span>
<span class="co">#&gt;  [988] 0.6397636 0.6397636 0.8374954 0.2570834 0.2362237 0.6134946 0.8374954</span>
<span class="co">#&gt;  [995] 0.1729661 0.8374954 0.6134946 0.2362237 0.7746227 0.7544128</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; $linear.predictors</span>
<span class="co">#&gt;    [1] -1.56475023  1.23460032  0.97944724  0.97944724  0.46202560  1.63970937</span>
<span class="co">#&gt;    [7]  0.86713465  0.46202560 -1.45243763 -1.56475023 -0.08363644  1.52739678</span>
<span class="co">#&gt;   [13] -1.06118299  0.97944724  0.46202560  1.23460032  1.23460032 -0.76838654</span>
<span class="co">#&gt;   [19] -1.45243763 -1.17349559  0.97944724 -1.56475023  0.46202560  1.12228773</span>
<span class="co">#&gt;   [25]  0.57433819  1.23460032  1.23460032 -1.56475023  0.57433819  0.46202560</span>
<span class="co">#&gt;   [31] -1.56475023  1.23460032  1.63970937  1.12228773  0.86713465 -1.31404858</span>
<span class="co">#&gt;   [37]  0.57433819  0.57433819  0.98173473  0.46202560 -1.45243763 -1.45243763</span>
<span class="co">#&gt;   [43]  1.23460032  0.46202560  0.57433819  0.57433819  1.52739678 -0.65607394</span>
<span class="co">#&gt;   [49]  0.57433819 -1.06118299  0.57433819 -1.45243763  1.12228773  0.57433819</span>
<span class="co">#&gt;   [55]  1.23460032  1.23460032 -1.06118299  0.57433819 -1.17349559  1.23460032</span>
<span class="co">#&gt;   [61] -1.45243763  0.46202560 -1.45243763 -1.04732858 -0.65607394 -1.71915763</span>
<span class="co">#&gt;   [67]  1.63970937  1.12228773  1.63970937 -1.06118299 -1.56475023  0.57433819</span>
<span class="co">#&gt;   [73]  0.97944724 -1.06118299 -1.45243763 -1.06118299  1.63970937 -1.04732858</span>
<span class="co">#&gt;   [79]  1.23460032 -1.45243763  1.12228773 -1.17349559  0.97944724 -1.56475023</span>
<span class="co">#&gt;   [85]  0.57433819  1.23460032  1.23460032 -1.06118299  1.23460032 -1.06118299</span>
<span class="co">#&gt;   [91] -1.06118299  1.12228773 -1.56475023 -1.45243763  1.63970937  1.23460032</span>
<span class="co">#&gt;   [97]  0.97944724  1.12228773 -1.06118299  1.63970937 -1.06118299  1.23460032</span>
<span class="co">#&gt;  [103]  0.97944724  0.46202560  1.23460032 -0.65607394 -1.06118299  1.23460032</span>
<span class="co">#&gt;  [109] -1.06118299 -1.04732858  1.23460032  0.46202560 -1.06118299  1.23460032</span>
<span class="co">#&gt;  [115]  0.57433819  0.97944724  0.46202560  1.23460032  1.23460032 -1.04732858</span>
<span class="co">#&gt;  [121]  1.23460032  1.23460032  0.46202560 -1.45243763  1.12228773  0.46202560</span>
<span class="co">#&gt;  [127]  1.12228773  0.86713465 -1.56475023 -1.06118299  1.23460032 -1.45243763</span>
<span class="co">#&gt;  [133]  1.23460032  1.52739678 -1.17349559  1.23460032 -1.06118299  1.63970937</span>
<span class="co">#&gt;  [139]  1.23460032 -1.06118299  1.23460032  0.46202560  1.23460032 -2.11041227</span>
<span class="co">#&gt;  [145] -1.06118299  1.23460032 -2.11041227 -1.56475023 -1.56475023 -1.06118299</span>
<span class="co">#&gt;  [151]  1.12228773  1.52739678  1.23460032  0.46202560 -1.06118299  1.23460032</span>
<span class="co">#&gt;  [157] -1.45243763  0.46202560  0.46202560  0.46202560  1.12228773 -0.65607394</span>
<span class="co">#&gt;  [163] -1.56475023  0.57433819 -1.06118299  1.52739678 -1.06118299  0.57433819</span>
<span class="co">#&gt;  [169]  0.57433819  0.57433819  0.46202560  0.46202560  0.57433819  1.23460032</span>
<span class="co">#&gt;  [175]  0.97944724  0.46202560  1.52739678  0.46202560 -1.45243763 -1.04732858</span>
<span class="co">#&gt;  [181] -1.45243763  0.57433819 -1.06118299 -2.11041227  1.63970937  1.63970937</span>
<span class="co">#&gt;  [187] -1.56475023  1.63970937 -1.45243763  0.57433819 -1.17349559 -0.76838654</span>
<span class="co">#&gt;  [193]  1.23460032  1.23460032 -1.45243763 -0.65607394  1.12228773  0.57433819</span>
<span class="co">#&gt;  [199] -1.06118299  1.23460032  1.63970937  1.63970937  1.23460032  1.52739678</span>
<span class="co">#&gt;  [205]  0.57433819  0.57433819  1.52739678 -1.17349559  1.12228773 -1.06118299</span>
<span class="co">#&gt;  [211]  0.46202560  0.57433819 -1.06118299  0.57433819  1.23460032 -1.06118299</span>
<span class="co">#&gt;  [217] -1.06118299  1.12228773 -1.15964118  1.12228773 -1.45243763  1.23460032</span>
<span class="co">#&gt;  [223] -1.15964118 -1.45243763 -0.65607394 -1.45243763 -1.17349559  0.46202560</span>
<span class="co">#&gt;  [229]  1.23460032  1.23460032 -1.06118299  1.12228773 -1.71915763  1.63970937</span>
<span class="co">#&gt;  [235]  1.63970937 -1.15964118  0.57433819  1.23460032  0.57433819  0.46202560</span>
<span class="co">#&gt;  [241]  1.12228773  0.57433819 -1.17349559 -1.06118299  0.46202560  1.23460032</span>
<span class="co">#&gt;  [247] -1.45243763 -0.08363644  0.57433819  1.23460032  0.57433819  1.23460032</span>
<span class="co">#&gt;  [253]  1.23460032  0.97944724  0.57433819  1.12228773  1.23460032  1.63970937</span>
<span class="co">#&gt;  [259]  1.12228773  0.57433819 -1.56475023  1.12228773  0.57433819 -1.45243763</span>
<span class="co">#&gt;  [265]  1.12228773 -1.06118299 -0.08363644  1.23460032  0.57433819 -1.04732858</span>
<span class="co">#&gt;  [271]  1.23460032  1.23460032  1.52739678  1.23460032  1.12228773 -1.06118299</span>
<span class="co">#&gt;  [277] -1.17349559  1.63970937  0.46202560  0.46202560  0.57433819 -1.06118299</span>
<span class="co">#&gt;  [283] -1.04732858  0.57433819  1.12228773  1.23460032  1.12228773 -0.76838654</span>
<span class="co">#&gt;  [289]  0.57433819  0.46202560 -0.08363644  0.57433819  1.23460032  0.46202560</span>
<span class="co">#&gt;  [295]  1.23460032 -1.06118299  0.46202560  1.23460032  1.63970937 -1.56475023</span>
<span class="co">#&gt;  [301]  1.23460032  1.12228773  0.57433819 -1.45243763  1.23460032  0.97944724</span>
<span class="co">#&gt;  [307]  1.23460032  1.63970937  1.23460032 -1.17349559 -1.06118299 -1.06118299</span>
<span class="co">#&gt;  [313] -1.06118299  0.46202560  1.23460032  0.46202560  0.97944724  0.46202560</span>
<span class="co">#&gt;  [319]  1.52739678  0.57433819  1.12228773  1.23460032  1.63970937  0.46202560</span>
<span class="co">#&gt;  [325]  0.57433819 -1.31404858  1.63970937  0.57433819 -1.06118299 -0.76838654</span>
<span class="co">#&gt;  [331] -0.65607394  1.12228773 -0.65607394  1.23460032 -1.04732858  1.12228773</span>
<span class="co">#&gt;  [337]  1.12228773  0.46202560  0.57433819 -1.56475023  1.12228773  1.23460032</span>
<span class="co">#&gt;  [343] -1.04732858 -1.04732858  1.23460032 -1.17349559  0.86713465  1.23460032</span>
<span class="co">#&gt;  [349] -1.31404858 -1.17349559  0.57433819  0.46202560  1.63970937  0.46202560</span>
<span class="co">#&gt;  [355]  1.23460032  1.23460032 -1.45243763  0.46202560 -1.45243763  1.63970937</span>
<span class="co">#&gt;  [361]  1.52739678 -1.45243763  0.46202560  1.12228773  0.46202560 -1.17349559</span>
<span class="co">#&gt;  [367]  1.63970937 -1.45243763 -1.06118299 -1.45243763 -0.65607394  0.46202560</span>
<span class="co">#&gt;  [373] -1.06118299  0.57433819  1.52739678 -1.06118299  1.12228773  0.97944724</span>
<span class="co">#&gt;  [379]  1.12228773  0.97944724 -1.06118299  0.57433819 -1.06118299  1.12228773</span>
<span class="co">#&gt;  [385] -1.56475023  0.46202560 -1.56475023  1.23460032 -1.45243763  1.23460032</span>
<span class="co">#&gt;  [391] -2.11041227  0.57433819  1.23460032  0.97944724  0.46202560 -1.06118299</span>
<span class="co">#&gt;  [397]  1.12228773  0.97944724 -1.45243763  0.97944724 -1.45243763  1.63970937</span>
<span class="co">#&gt;  [403]  1.23460032  0.97944724 -1.06118299 -0.65607394  1.12228773  1.23460032</span>
<span class="co">#&gt;  [409] -1.06118299 -0.08363644  0.46202560 -1.56475023 -1.45243763  0.57433819</span>
<span class="co">#&gt;  [415]  0.46202560 -1.06118299  1.52739678  1.23460032  1.23460032 -0.65607394</span>
<span class="co">#&gt;  [421]  1.52739678 -1.45243763 -1.17349559  1.12228773  0.46202560  1.12228773</span>
<span class="co">#&gt;  [427] -1.56475023 -0.65607394  0.57433819 -1.45243763  0.46202560 -1.06118299</span>
<span class="co">#&gt;  [433] -1.17349559 -0.76838654  0.57433819 -1.06118299  1.23460032 -1.06118299</span>
<span class="co">#&gt;  [439]  0.57433819  0.97944724  1.23460032  0.46202560  1.12228773 -1.06118299</span>
<span class="co">#&gt;  [445] -1.06118299  1.23460032  1.23460032  0.57433819  1.23460032 -1.17349559</span>
<span class="co">#&gt;  [451]  0.57433819  1.23460032 -1.56475023  0.46202560  1.23460032  0.46202560</span>
<span class="co">#&gt;  [457]  0.57433819  0.46202560  1.12228773  1.12228773  1.23460032 -1.45243763</span>
<span class="co">#&gt;  [463]  1.23460032 -1.06118299 -1.56475023 -0.76838654 -1.06118299  0.46202560</span>
<span class="co">#&gt;  [469]  1.23460032  1.23460032  1.63970937 -0.76838654 -1.17349559  0.57433819</span>
<span class="co">#&gt;  [475] -1.06118299  0.57433819  1.63970937  0.57433819  0.46202560 -1.04732858</span>
<span class="co">#&gt;  [481] -0.65607394 -1.45243763  1.63970937  1.63970937  0.46202560  1.52739678</span>
<span class="co">#&gt;  [487] -1.06118299  1.63970937  1.23460032  0.46202560 -1.45243763  0.57433819</span>
<span class="co">#&gt;  [493]  0.57662568 -0.65607394  1.23460032  0.57433819 -1.45243763  1.12228773</span>
<span class="co">#&gt;  [499]  1.23460032 -1.06118299  0.46202560  1.63970937 -0.65607394 -1.45243763</span>
<span class="co">#&gt;  [505] -1.06118299 -1.17349559  0.57433819  1.12228773  0.57433819  1.23460032</span>
<span class="co">#&gt;  [511]  1.12228773 -1.06118299 -1.56475023  0.46202560  0.57433819  1.12228773</span>
<span class="co">#&gt;  [517]  0.57433819  1.23460032  0.57433819  1.52739678  0.46202560  1.23460032</span>
<span class="co">#&gt;  [523]  0.46202560  0.57433819 -0.08363644 -1.17349559  0.57433819  0.46202560</span>
<span class="co">#&gt;  [529] -1.17349559  1.23460032  1.23460032  1.52739678  1.23460032 -1.06118299</span>
<span class="co">#&gt;  [535]  1.63970937  0.46202560  0.46202560  0.86713465  1.23460032 -1.17349559</span>
<span class="co">#&gt;  [541]  0.97944724  1.23460032  1.23460032  1.23460032 -1.06118299 -1.06118299</span>
<span class="co">#&gt;  [547] -0.76838654  0.57433819  1.23460032  1.63970937  0.97944724 -0.76838654</span>
<span class="co">#&gt;  [553]  1.23460032  0.97944724 -1.06118299  0.57433819  1.12228773 -1.06118299</span>
<span class="co">#&gt;  [559]  0.57433819 -0.76838654  0.57433819  0.57433819  1.23460032 -1.04732858</span>
<span class="co">#&gt;  [565]  0.57433819  0.57433819  1.23460032  1.52739678  1.63970937 -0.65607394</span>
<span class="co">#&gt;  [571] -0.08363644 -1.04732858  0.57433819 -1.45243763 -1.17349559 -1.56475023</span>
<span class="co">#&gt;  [577] -1.06118299  0.57433819  1.23460032  0.57433819 -1.06118299  1.12228773</span>
<span class="co">#&gt;  [583]  0.97944724 -1.04732858  0.57433819  0.57433819  1.52739678  1.63970937</span>
<span class="co">#&gt;  [589]  1.23460032  0.57433819  1.12228773  0.97944724  0.57433819 -1.04732858</span>
<span class="co">#&gt;  [595]  0.46202560  0.97944724  1.12228773  1.23460032 -1.06118299 -1.06118299</span>
<span class="co">#&gt;  [601]  0.57433819  0.97944724  1.12228773 -1.45243763 -1.17349559  1.52739678</span>
<span class="co">#&gt;  [607]  0.97944724  1.23460032  1.23460032  0.86713465 -0.65607394  0.57433819</span>
<span class="co">#&gt;  [613] -1.06118299  0.57433819  1.12228773  0.57433819  1.12228773  0.46202560</span>
<span class="co">#&gt;  [619]  0.46202560  0.46202560  1.12228773  0.46202560 -1.15964118  1.23460032</span>
<span class="co">#&gt;  [625] -0.65607394  1.23460032 -1.17349559 -1.45243763  1.23460032  1.23460032</span>
<span class="co">#&gt;  [631]  1.23460032  0.57433819 -0.08363644 -1.06118299 -1.06118299  0.86713465</span>
<span class="co">#&gt;  [637] -1.56475023  0.46202560  1.23460032  0.57433819  1.23460032  1.12228773</span>
<span class="co">#&gt;  [643]  0.57433819  1.12228773 -1.17349559  1.23460032 -1.06118299 -0.08363644</span>
<span class="co">#&gt;  [649]  1.63970937 -1.04732858 -1.06118299 -1.06118299  0.57433819  1.12228773</span>
<span class="co">#&gt;  [655] -0.65607394  1.12228773  0.57433819  1.23460032  1.23460032 -1.06118299</span>
<span class="co">#&gt;  [661] -1.06118299  1.63970937 -1.06118299 -1.04732858 -1.17349559  0.46202560</span>
<span class="co">#&gt;  [667]  0.57662568  0.57433819 -0.76838654  1.12228773 -1.17349559  1.12228773</span>
<span class="co">#&gt;  [673] -1.17349559  0.86713465  0.57433819  0.57433819  1.23460032  0.57433819</span>
<span class="co">#&gt;  [679]  1.23460032  0.57433819  0.46202560 -0.65607394  1.12228773  1.63970937</span>
<span class="co">#&gt;  [685]  1.52739678  1.23460032  0.57433819  0.57433819  0.57433819  1.12228773</span>
<span class="co">#&gt;  [691] -1.17349559  0.57433819  1.23460032  1.23460032  0.86713465  0.46202560</span>
<span class="co">#&gt;  [697]  0.46202560  0.97944724 -0.76838654 -1.45243763 -1.06118299  0.97944724</span>
<span class="co">#&gt;  [703]  1.23460032  1.12228773 -1.17349559  1.23460032  1.23460032  1.12228773</span>
<span class="co">#&gt;  [709]  0.57433819 -1.71915763  0.57433819  1.12228773  1.52739678 -1.56475023</span>
<span class="co">#&gt;  [715] -1.06118299  0.97944724  1.63970937  0.57433819  0.57433819  0.57433819</span>
<span class="co">#&gt;  [721]  0.46202560  1.23460032 -1.06118299  1.23460032 -1.06118299  0.97944724</span>
<span class="co">#&gt;  [727]  0.57433819  0.46202560  1.12228773  0.57433819 -1.06118299  1.63970937</span>
<span class="co">#&gt;  [733] -1.04732858 -1.06118299  0.46202560  0.46202560  0.46202560  1.12228773</span>
<span class="co">#&gt;  [739] -0.76838654 -1.06118299 -0.76838654  0.57433819 -1.06118299  1.23460032</span>
<span class="co">#&gt;  [745] -0.76838654  1.23460032 -1.56475023  1.23460032  1.23460032  0.97944724</span>
<span class="co">#&gt;  [751] -1.45243763  1.23460032  1.23460032 -1.04732858  1.23460032 -1.45243763</span>
<span class="co">#&gt;  [757]  1.12228773  0.57662568  1.23460032 -1.45243763 -1.06118299  0.57433819</span>
<span class="co">#&gt;  [763] -1.06118299  1.23460032  0.97944724  0.97944724 -0.65607394  0.46202560</span>
<span class="co">#&gt;  [769] -1.45243763  0.57433819  1.23460032  0.57433819 -1.06118299  0.57433819</span>
<span class="co">#&gt;  [775] -0.76838654  1.12228773 -1.45243763 -0.65607394 -1.06118299  1.23460032</span>
<span class="co">#&gt;  [781] -1.06118299  1.23460032 -0.65607394  0.57433819  1.23460032  0.57433819</span>
<span class="co">#&gt;  [787] -1.04732858 -0.08363644  1.23460032  0.57433819  0.46202560 -1.56475023</span>
<span class="co">#&gt;  [793]  1.23460032  1.12228773  1.23460032  0.57433819  1.23460032  0.57433819</span>
<span class="co">#&gt;  [799]  1.23460032  0.57433819  1.23460032 -1.06118299 -1.45243763  0.46202560</span>
<span class="co">#&gt;  [805]  1.23460032  0.57433819 -1.71915763 -1.06118299 -2.11041227  1.23460032</span>
<span class="co">#&gt;  [811] -1.06118299 -1.06118299 -1.06118299  0.97944724 -1.45243763 -0.65607394</span>
<span class="co">#&gt;  [817]  0.97944724 -1.06118299  0.57662568  0.98173473 -1.56475023  1.23460032</span>
<span class="co">#&gt;  [823]  0.57433819  1.23460032  0.46202560  1.63970937  0.57433819 -1.06118299</span>
<span class="co">#&gt;  [829]  1.23460032 -0.76838654 -1.06118299  1.12228773  1.23460032 -0.65607394</span>
<span class="co">#&gt;  [835]  0.57433819 -1.06118299 -1.15964118  0.57433819  1.23460032  0.57433819</span>
<span class="co">#&gt;  [841] -0.08363644  1.23460032  0.57433819 -1.17349559  0.46202560  0.57433819</span>
<span class="co">#&gt;  [847] -1.45243763  1.63970937  1.23460032 -1.56475023  0.57433819  1.23460032</span>
<span class="co">#&gt;  [853]  0.46202560  0.97944724  1.23460032  0.97944724  1.23460032 -2.11041227</span>
<span class="co">#&gt;  [859]  1.12228773 -1.56475023  1.12228773  1.63970937  1.23460032  1.23460032</span>
<span class="co">#&gt;  [865]  0.57433819  1.23460032  1.63970937  0.97944724 -0.65607394  1.23460032</span>
<span class="co">#&gt;  [871] -1.45243763  1.23460032  0.46202560 -0.76838654  0.46202560  1.23460032</span>
<span class="co">#&gt;  [877] -1.06118299  0.57433819  0.97944724 -1.15964118 -1.45243763 -1.06118299</span>
<span class="co">#&gt;  [883] -1.71915763 -1.71915763  0.57433819  0.46202560 -1.17349559  1.63970937</span>
<span class="co">#&gt;  [889] -1.06118299  1.23460032  1.23460032 -1.17349559 -1.17349559  1.52739678</span>
<span class="co">#&gt;  [895] -0.65607394  0.46202560  0.46202560 -1.17349559  0.57433819  0.86713465</span>
<span class="co">#&gt;  [901] -1.06118299 -1.56475023 -1.45243763 -0.76838654  0.46202560 -1.04732858</span>
<span class="co">#&gt;  [907]  1.23460032  1.23460032  1.23460032  0.57433819  1.23460032 -1.45243763</span>
<span class="co">#&gt;  [913] -1.17349559 -1.04732858  1.23460032 -1.45243763  0.57433819  1.23460032</span>
<span class="co">#&gt;  [919] -1.06118299  0.57433819  1.23460032  1.23460032 -1.17349559  0.57433819</span>
<span class="co">#&gt;  [925] -1.06118299  1.63970937 -1.17349559  1.23460032 -1.06118299  0.46202560</span>
<span class="co">#&gt;  [931]  1.23460032  1.12228773 -1.45243763  0.46202560  1.23460032  0.57433819</span>
<span class="co">#&gt;  [937] -1.71915763  1.12228773  1.63970937 -0.65607394 -1.06118299  0.57662568</span>
<span class="co">#&gt;  [943]  1.12228773  1.63970937  1.23460032  1.12228773 -1.17349559 -1.17349559</span>
<span class="co">#&gt;  [949]  0.46202560  0.57433819  0.57433819  0.46202560  0.57433819  1.12228773</span>
<span class="co">#&gt;  [955] -1.45243763  1.23460032  1.63970937  1.63970937  1.12228773 -1.06118299</span>
<span class="co">#&gt;  [961] -1.06118299  0.98173473 -1.15964118  1.12228773 -1.04732858  1.23460032</span>
<span class="co">#&gt;  [967] -1.04732858 -1.45243763  1.23460032  0.97944724  0.46202560 -1.06118299</span>
<span class="co">#&gt;  [973]  0.57433819  1.23460032  0.57433819  0.97944724 -1.45243763 -1.45243763</span>
<span class="co">#&gt;  [979]  0.57433819  1.63970937  0.86713465  0.97944724 -1.56475023  1.63970937</span>
<span class="co">#&gt;  [985] -1.06118299  0.57433819  1.23460032  0.57433819  0.57433819  1.63970937</span>
<span class="co">#&gt;  [991] -1.06118299 -1.17349559  0.46202560  1.63970937 -1.56475023  1.63970937</span>
<span class="co">#&gt;  [997]  0.46202560 -1.17349559  1.23460032  1.12228773</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; $loglikelihood</span>
<span class="co">#&gt; [1] -567.9048</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; $converged</span>
<span class="co">#&gt; [1] TRUE</span>

<span class="co"># The first link function is:</span>
<span class="no">discretization</span>@<span class="kw">best.disc</span><span class="kw">[[</span><span class="fl">2</span>]]<span class="kw">[[</span><span class="fl">1</span>]]
<span class="co">#&gt; Call:</span>
<span class="co">#&gt; nnet::multinom(formula = e ~ x, data = data.frame(e = e[continu_complete_case[, </span>
<span class="co">#&gt;     j] &amp; ensemble[[1]], j], x = predictors[continu_complete_case[, </span>
<span class="co">#&gt;     j] &amp; ensemble[[1]], j], stringsAsFactors = TRUE), Hess = FALSE, </span>
<span class="co">#&gt;     start = link[[j]]$coefficients, trace = FALSE, maxit = 50)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Coefficients:</span>
<span class="co">#&gt;   (Intercept)          x</span>
<span class="co">#&gt; 2   6.7965587 -18.518803</span>
<span class="co">#&gt; 3   1.9920277  -3.641154</span>
<span class="co">#&gt; 4   0.2691619  -1.701950</span>
<span class="co">#&gt; 5  -4.8437781   7.208232</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Residual Deviance: 1915.623 </span>
<span class="co">#&gt; AIC: 1931.623</span></pre></body></html></div>
</div>
<div id="the-performance-slot" class="section level3">
<h3 class="hasAnchor">
<a href="#the-performance-slot" class="anchor"></a>The <code>performance</code> slot</h3>
<p>The <code>performance</code> slot lists both the achieved performance defined by the parameters <code>criterion</code>, <code>test</code> and <code>validation</code> and this performance metric over all the iterations.</p>
<p><code>discretization</code> was fit with no test nor validation and we used the AIC criterion. The best AIC achieved is therefore:</p>
<div class="sourceCode" id="cb10"><html><body><pre class="r"><span class="no">discretization</span>@<span class="kw">performance</span><span class="kw">[[</span><span class="fl">1</span>]]
<span class="co">#&gt; [1] 0.5130532</span></pre></body></html></div>
<p>The first 5 AIC values obtained on the training set over the iterations are:</p>
<div class="sourceCode" id="cb11"><html><body><pre class="r"><span class="no">discretization</span>@<span class="kw">performance</span><span class="kw">[[</span><span class="fl">2</span>]][<span class="fl">1</span>:<span class="fl">5</span>]
<span class="co">#&gt; [[1]]</span>
<span class="co">#&gt; [1] -Inf</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; [[2]]</span>
<span class="co">#&gt; [1] -1296.277</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; [[3]]</span>
<span class="co">#&gt; [1] -1328.67</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; [[4]]</span>
<span class="co">#&gt; [1] -1364.882</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; [[5]]</span>
<span class="co">#&gt; [1] -1376.697</span></pre></body></html></div>
</div>
<div id="the-disc-data-slot" class="section level3">
<h3 class="hasAnchor">
<a href="#the-disc-data-slot" class="anchor"></a>The <code>disc.data</code> slot</h3>
<p>The <code>disc.data</code> slot contains the discretized data obtained by applying the best discretization scheme found on the test set if <code>test=TRUE</code> or the validation set if <code>validation=TRUE</code> or the training set.</p>
<p>In our example, <code>discretization</code> has <code>validation=FALSE</code> and <code>test=FALSE</code> so that we get the discretized training set:</p>
<div class="sourceCode" id="cb12"><html><body><pre class="r"><span class="no">discretization</span>@<span class="kw">disc.data</span></pre></body></html></div>
<table class="table">
<thead><tr class="header">
<th align="left">V1</th>
<th align="left">V2</th>
<th align="left">V3</th>
<th align="left">labels</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">1</td>
<td align="left">3</td>
<td align="left">4</td>
<td align="left">1</td>
</tr>
<tr class="even">
<td align="left">2</td>
<td align="left">3</td>
<td align="left">1</td>
<td align="left">1</td>
</tr>
<tr class="odd">
<td align="left">5</td>
<td align="left">4</td>
<td align="left">1</td>
<td align="left">1</td>
</tr>
<tr class="even">
<td align="left">5</td>
<td align="left">4</td>
<td align="left">1</td>
<td align="left">1</td>
</tr>
<tr class="odd">
<td align="left">5</td>
<td align="left">3</td>
<td align="left">4</td>
<td align="left">1</td>
</tr>
<tr class="even">
<td align="left">2</td>
<td align="left">4</td>
<td align="left">1</td>
<td align="left">1</td>
</tr>
</tbody>
</table>
</div>
<div id="the-cont-data-slot" class="section level3">
<h3 class="hasAnchor">
<a href="#the-cont-data-slot" class="anchor"></a>The <code>cont.data</code> slot</h3>
<p>The <code>cont.data</code> slot contains the original “continuous” counterpart of the <code>disc.data</code> slot.</p>
<p>In our example, <code>discretization</code> has <code>validation=FALSE</code> and <code>test=FALSE</code> so that we get the “raw” training set:</p>
<div class="sourceCode" id="cb13"><html><body><pre class="r"><span class="no">discretization</span>@<span class="kw">cont.data</span></pre></body></html></div>
<table class="table">
<thead><tr class="header">
<th align="right">V1</th>
<th align="right">V2</th>
<th align="right">V3</th>
<th align="right">labels</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="right">0.6145490</td>
<td align="right">0.6258488</td>
<td align="right">0.9705037</td>
<td align="right">1</td>
</tr>
<tr class="even">
<td align="right">0.2181264</td>
<td align="right">0.1016004</td>
<td align="right">0.0863407</td>
<td align="right">1</td>
</tr>
<tr class="odd">
<td align="right">0.9175628</td>
<td align="right">0.9687333</td>
<td align="right">0.5527685</td>
<td align="right">1</td>
</tr>
<tr class="even">
<td align="right">0.7253785</td>
<td align="right">0.8021687</td>
<td align="right">0.3790259</td>
<td align="right">1</td>
</tr>
<tr class="odd">
<td align="right">0.7418341</td>
<td align="right">0.1886790</td>
<td align="right">0.9901026</td>
<td align="right">1</td>
</tr>
<tr class="even">
<td align="right">0.1301252</td>
<td align="right">0.8412636</td>
<td align="right">0.1827299</td>
<td align="right">1</td>
</tr>
</tbody>
</table>
</div>
<div id="how-well-did-we-do" class="section level3">
<h3 class="hasAnchor">
<a href="#how-well-did-we-do" class="anchor"></a>How well did we do?</h3>
<p>To compare the estimated and the true discretization schemes, we can represent them with respect to the input “raw” data <code>x</code>: <!--```{r, echo=TRUE, out.width='.49\\linewidth', fig.width=3, fig.height=3,fig.show='hold'}--> <img src="glmdisc_files/figure-html/unnamed-chunk-21-1.png" width="672" style="display: block; margin: auto;"><img src="glmdisc_files/figure-html/unnamed-chunk-21-2.png" width="672" style="display: block; margin: auto;"></p>
</div>
</div>
<div id="classical-methods" class="section level2">
<h2 class="hasAnchor">
<a href="#classical-methods" class="anchor"></a>“Classical” methods</h2>
<p>A few classical R methods have been adapted to the <code>glmdisc</code> S4 object.</p>
<!-- The `plot` function is the standard `plot.glm` method applied to the best discretization scheme found by the `glmdisc` function: -->
<!-- ```{r, echo=TRUE} -->
<!-- # plot(discretization) -->
<!-- ``` -->
<p>The <code>print</code> function is the standard <code>print.summary.glm</code> method applied to the best discretization scheme found by the <code>glmdisc</code> function:</p>
<div class="sourceCode" id="cb14"><html><body><pre class="r"><span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span>(<span class="no">discretization</span>)
<span class="co">#&gt; $coefficients</span>
<span class="co">#&gt; [1] -1.32943135  1.35760660 -0.93817671  0.69734447 -0.30888302  0.09622603</span>
<span class="co">#&gt; [7]  0.18587674 -0.47209789  0.07356415</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; $fitted.values</span>
<span class="co">#&gt;    [1] 0.1729661 0.7746227 0.7269985 0.7269985 0.6134946 0.8374954 0.7041491</span>
<span class="co">#&gt;    [8] 0.6134946 0.1896267 0.1729661 0.4791031 0.8216251 0.2570834 0.7269985</span>
<span class="co">#&gt;   [15] 0.6134946 0.7746227 0.7746227 0.3168282 0.1896267 0.2362237 0.7269985</span>
<span class="co">#&gt;   [22] 0.1729661 0.6134946 0.7544128 0.6397636 0.7746227 0.7746227 0.1729661</span>
<span class="co">#&gt;   [29] 0.6397636 0.6134946 0.1729661 0.7746227 0.8374954 0.7544128 0.7041491</span>
<span class="co">#&gt;   [36] 0.2118102 0.6397636 0.6397636 0.7274523 0.6134946 0.1896267 0.1896267</span>
<span class="co">#&gt;   [43] 0.7746227 0.6134946 0.6397636 0.6397636 0.8216251 0.3416221 0.6397636</span>
<span class="co">#&gt;   [50] 0.2570834 0.6397636 0.1896267 0.7544128 0.6397636 0.7746227 0.7746227</span>
<span class="co">#&gt;   [57] 0.2570834 0.6397636 0.2362237 0.7746227 0.1896267 0.6134946 0.1896267</span>
<span class="co">#&gt;   [64] 0.2597384 0.3416221 0.1519797 0.8374954 0.7544128 0.8374954 0.2570834</span>
<span class="co">#&gt;   [71] 0.1729661 0.6397636 0.7269985 0.2570834 0.1896267 0.2570834 0.8374954</span>
<span class="co">#&gt;   [78] 0.2597384 0.7746227 0.1896267 0.7544128 0.2362237 0.7269985 0.1729661</span>
<span class="co">#&gt;   [85] 0.6397636 0.7746227 0.7746227 0.2570834 0.7746227 0.2570834 0.2570834</span>
<span class="co">#&gt;   [92] 0.7544128 0.1729661 0.1896267 0.8374954 0.7746227 0.7269985 0.7544128</span>
<span class="co">#&gt;   [99] 0.2570834 0.8374954 0.2570834 0.7746227 0.7269985 0.6134946 0.7746227</span>
<span class="co">#&gt;  [106] 0.3416221 0.2570834 0.7746227 0.2570834 0.2597384 0.7746227 0.6134946</span>
<span class="co">#&gt;  [113] 0.2570834 0.7746227 0.6397636 0.7269985 0.6134946 0.7746227 0.7746227</span>
<span class="co">#&gt;  [120] 0.2597384 0.7746227 0.7746227 0.6134946 0.1896267 0.7544128 0.6134946</span>
<span class="co">#&gt;  [127] 0.7544128 0.7041491 0.1729661 0.2570834 0.7746227 0.1896267 0.7746227</span>
<span class="co">#&gt;  [134] 0.8216251 0.2362237 0.7746227 0.2570834 0.8374954 0.7746227 0.2570834</span>
<span class="co">#&gt;  [141] 0.7746227 0.6134946 0.7746227 0.1080889 0.2570834 0.7746227 0.1080889</span>
<span class="co">#&gt;  [148] 0.1729661 0.1729661 0.2570834 0.7544128 0.8216251 0.7746227 0.6134946</span>
<span class="co">#&gt;  [155] 0.2570834 0.7746227 0.1896267 0.6134946 0.6134946 0.6134946 0.7544128</span>
<span class="co">#&gt;  [162] 0.3416221 0.1729661 0.6397636 0.2570834 0.8216251 0.2570834 0.6397636</span>
<span class="co">#&gt;  [169] 0.6397636 0.6397636 0.6134946 0.6134946 0.6397636 0.7746227 0.7269985</span>
<span class="co">#&gt;  [176] 0.6134946 0.8216251 0.6134946 0.1896267 0.2597384 0.1896267 0.6397636</span>
<span class="co">#&gt;  [183] 0.2570834 0.1080889 0.8374954 0.8374954 0.1729661 0.8374954 0.1896267</span>
<span class="co">#&gt;  [190] 0.6397636 0.2362237 0.3168282 0.7746227 0.7746227 0.1896267 0.3416221</span>
<span class="co">#&gt;  [197] 0.7544128 0.6397636 0.2570834 0.7746227 0.8374954 0.8374954 0.7746227</span>
<span class="co">#&gt;  [204] 0.8216251 0.6397636 0.6397636 0.8216251 0.2362237 0.7544128 0.2570834</span>
<span class="co">#&gt;  [211] 0.6134946 0.6397636 0.2570834 0.6397636 0.7746227 0.2570834 0.2570834</span>
<span class="co">#&gt;  [218] 0.7544128 0.2387325 0.7544128 0.1896267 0.7746227 0.2387325 0.1896267</span>
<span class="co">#&gt;  [225] 0.3416221 0.1896267 0.2362237 0.6134946 0.7746227 0.7746227 0.2570834</span>
<span class="co">#&gt;  [232] 0.7544128 0.1519797 0.8374954 0.8374954 0.2387325 0.6397636 0.7746227</span>
<span class="co">#&gt;  [239] 0.6397636 0.6134946 0.7544128 0.6397636 0.2362237 0.2570834 0.6134946</span>
<span class="co">#&gt;  [246] 0.7746227 0.1896267 0.4791031 0.6397636 0.7746227 0.6397636 0.7746227</span>
<span class="co">#&gt;  [253] 0.7746227 0.7269985 0.6397636 0.7544128 0.7746227 0.8374954 0.7544128</span>
<span class="co">#&gt;  [260] 0.6397636 0.1729661 0.7544128 0.6397636 0.1896267 0.7544128 0.2570834</span>
<span class="co">#&gt;  [267] 0.4791031 0.7746227 0.6397636 0.2597384 0.7746227 0.7746227 0.8216251</span>
<span class="co">#&gt;  [274] 0.7746227 0.7544128 0.2570834 0.2362237 0.8374954 0.6134946 0.6134946</span>
<span class="co">#&gt;  [281] 0.6397636 0.2570834 0.2597384 0.6397636 0.7544128 0.7746227 0.7544128</span>
<span class="co">#&gt;  [288] 0.3168282 0.6397636 0.6134946 0.4791031 0.6397636 0.7746227 0.6134946</span>
<span class="co">#&gt;  [295] 0.7746227 0.2570834 0.6134946 0.7746227 0.8374954 0.1729661 0.7746227</span>
<span class="co">#&gt;  [302] 0.7544128 0.6397636 0.1896267 0.7746227 0.7269985 0.7746227 0.8374954</span>
<span class="co">#&gt;  [309] 0.7746227 0.2362237 0.2570834 0.2570834 0.2570834 0.6134946 0.7746227</span>
<span class="co">#&gt;  [316] 0.6134946 0.7269985 0.6134946 0.8216251 0.6397636 0.7544128 0.7746227</span>
<span class="co">#&gt;  [323] 0.8374954 0.6134946 0.6397636 0.2118102 0.8374954 0.6397636 0.2570834</span>
<span class="co">#&gt;  [330] 0.3168282 0.3416221 0.7544128 0.3416221 0.7746227 0.2597384 0.7544128</span>
<span class="co">#&gt;  [337] 0.7544128 0.6134946 0.6397636 0.1729661 0.7544128 0.7746227 0.2597384</span>
<span class="co">#&gt;  [344] 0.2597384 0.7746227 0.2362237 0.7041491 0.7746227 0.2118102 0.2362237</span>
<span class="co">#&gt;  [351] 0.6397636 0.6134946 0.8374954 0.6134946 0.7746227 0.7746227 0.1896267</span>
<span class="co">#&gt;  [358] 0.6134946 0.1896267 0.8374954 0.8216251 0.1896267 0.6134946 0.7544128</span>
<span class="co">#&gt;  [365] 0.6134946 0.2362237 0.8374954 0.1896267 0.2570834 0.1896267 0.3416221</span>
<span class="co">#&gt;  [372] 0.6134946 0.2570834 0.6397636 0.8216251 0.2570834 0.7544128 0.7269985</span>
<span class="co">#&gt;  [379] 0.7544128 0.7269985 0.2570834 0.6397636 0.2570834 0.7544128 0.1729661</span>
<span class="co">#&gt;  [386] 0.6134946 0.1729661 0.7746227 0.1896267 0.7746227 0.1080889 0.6397636</span>
<span class="co">#&gt;  [393] 0.7746227 0.7269985 0.6134946 0.2570834 0.7544128 0.7269985 0.1896267</span>
<span class="co">#&gt;  [400] 0.7269985 0.1896267 0.8374954 0.7746227 0.7269985 0.2570834 0.3416221</span>
<span class="co">#&gt;  [407] 0.7544128 0.7746227 0.2570834 0.4791031 0.6134946 0.1729661 0.1896267</span>
<span class="co">#&gt;  [414] 0.6397636 0.6134946 0.2570834 0.8216251 0.7746227 0.7746227 0.3416221</span>
<span class="co">#&gt;  [421] 0.8216251 0.1896267 0.2362237 0.7544128 0.6134946 0.7544128 0.1729661</span>
<span class="co">#&gt;  [428] 0.3416221 0.6397636 0.1896267 0.6134946 0.2570834 0.2362237 0.3168282</span>
<span class="co">#&gt;  [435] 0.6397636 0.2570834 0.7746227 0.2570834 0.6397636 0.7269985 0.7746227</span>
<span class="co">#&gt;  [442] 0.6134946 0.7544128 0.2570834 0.2570834 0.7746227 0.7746227 0.6397636</span>
<span class="co">#&gt;  [449] 0.7746227 0.2362237 0.6397636 0.7746227 0.1729661 0.6134946 0.7746227</span>
<span class="co">#&gt;  [456] 0.6134946 0.6397636 0.6134946 0.7544128 0.7544128 0.7746227 0.1896267</span>
<span class="co">#&gt;  [463] 0.7746227 0.2570834 0.1729661 0.3168282 0.2570834 0.6134946 0.7746227</span>
<span class="co">#&gt;  [470] 0.7746227 0.8374954 0.3168282 0.2362237 0.6397636 0.2570834 0.6397636</span>
<span class="co">#&gt;  [477] 0.8374954 0.6397636 0.6134946 0.2597384 0.3416221 0.1896267 0.8374954</span>
<span class="co">#&gt;  [484] 0.8374954 0.6134946 0.8216251 0.2570834 0.8374954 0.7746227 0.6134946</span>
<span class="co">#&gt;  [491] 0.1896267 0.6397636 0.6402906 0.3416221 0.7746227 0.6397636 0.1896267</span>
<span class="co">#&gt;  [498] 0.7544128 0.7746227 0.2570834 0.6134946 0.8374954 0.3416221 0.1896267</span>
<span class="co">#&gt;  [505] 0.2570834 0.2362237 0.6397636 0.7544128 0.6397636 0.7746227 0.7544128</span>
<span class="co">#&gt;  [512] 0.2570834 0.1729661 0.6134946 0.6397636 0.7544128 0.6397636 0.7746227</span>
<span class="co">#&gt;  [519] 0.6397636 0.8216251 0.6134946 0.7746227 0.6134946 0.6397636 0.4791031</span>
<span class="co">#&gt;  [526] 0.2362237 0.6397636 0.6134946 0.2362237 0.7746227 0.7746227 0.8216251</span>
<span class="co">#&gt;  [533] 0.7746227 0.2570834 0.8374954 0.6134946 0.6134946 0.7041491 0.7746227</span>
<span class="co">#&gt;  [540] 0.2362237 0.7269985 0.7746227 0.7746227 0.7746227 0.2570834 0.2570834</span>
<span class="co">#&gt;  [547] 0.3168282 0.6397636 0.7746227 0.8374954 0.7269985 0.3168282 0.7746227</span>
<span class="co">#&gt;  [554] 0.7269985 0.2570834 0.6397636 0.7544128 0.2570834 0.6397636 0.3168282</span>
<span class="co">#&gt;  [561] 0.6397636 0.6397636 0.7746227 0.2597384 0.6397636 0.6397636 0.7746227</span>
<span class="co">#&gt;  [568] 0.8216251 0.8374954 0.3416221 0.4791031 0.2597384 0.6397636 0.1896267</span>
<span class="co">#&gt;  [575] 0.2362237 0.1729661 0.2570834 0.6397636 0.7746227 0.6397636 0.2570834</span>
<span class="co">#&gt;  [582] 0.7544128 0.7269985 0.2597384 0.6397636 0.6397636 0.8216251 0.8374954</span>
<span class="co">#&gt;  [589] 0.7746227 0.6397636 0.7544128 0.7269985 0.6397636 0.2597384 0.6134946</span>
<span class="co">#&gt;  [596] 0.7269985 0.7544128 0.7746227 0.2570834 0.2570834 0.6397636 0.7269985</span>
<span class="co">#&gt;  [603] 0.7544128 0.1896267 0.2362237 0.8216251 0.7269985 0.7746227 0.7746227</span>
<span class="co">#&gt;  [610] 0.7041491 0.3416221 0.6397636 0.2570834 0.6397636 0.7544128 0.6397636</span>
<span class="co">#&gt;  [617] 0.7544128 0.6134946 0.6134946 0.6134946 0.7544128 0.6134946 0.2387325</span>
<span class="co">#&gt;  [624] 0.7746227 0.3416221 0.7746227 0.2362237 0.1896267 0.7746227 0.7746227</span>
<span class="co">#&gt;  [631] 0.7746227 0.6397636 0.4791031 0.2570834 0.2570834 0.7041491 0.1729661</span>
<span class="co">#&gt;  [638] 0.6134946 0.7746227 0.6397636 0.7746227 0.7544128 0.6397636 0.7544128</span>
<span class="co">#&gt;  [645] 0.2362237 0.7746227 0.2570834 0.4791031 0.8374954 0.2597384 0.2570834</span>
<span class="co">#&gt;  [652] 0.2570834 0.6397636 0.7544128 0.3416221 0.7544128 0.6397636 0.7746227</span>
<span class="co">#&gt;  [659] 0.7746227 0.2570834 0.2570834 0.8374954 0.2570834 0.2597384 0.2362237</span>
<span class="co">#&gt;  [666] 0.6134946 0.6402906 0.6397636 0.3168282 0.7544128 0.2362237 0.7544128</span>
<span class="co">#&gt;  [673] 0.2362237 0.7041491 0.6397636 0.6397636 0.7746227 0.6397636 0.7746227</span>
<span class="co">#&gt;  [680] 0.6397636 0.6134946 0.3416221 0.7544128 0.8374954 0.8216251 0.7746227</span>
<span class="co">#&gt;  [687] 0.6397636 0.6397636 0.6397636 0.7544128 0.2362237 0.6397636 0.7746227</span>
<span class="co">#&gt;  [694] 0.7746227 0.7041491 0.6134946 0.6134946 0.7269985 0.3168282 0.1896267</span>
<span class="co">#&gt;  [701] 0.2570834 0.7269985 0.7746227 0.7544128 0.2362237 0.7746227 0.7746227</span>
<span class="co">#&gt;  [708] 0.7544128 0.6397636 0.1519797 0.6397636 0.7544128 0.8216251 0.1729661</span>
<span class="co">#&gt;  [715] 0.2570834 0.7269985 0.8374954 0.6397636 0.6397636 0.6397636 0.6134946</span>
<span class="co">#&gt;  [722] 0.7746227 0.2570834 0.7746227 0.2570834 0.7269985 0.6397636 0.6134946</span>
<span class="co">#&gt;  [729] 0.7544128 0.6397636 0.2570834 0.8374954 0.2597384 0.2570834 0.6134946</span>
<span class="co">#&gt;  [736] 0.6134946 0.6134946 0.7544128 0.3168282 0.2570834 0.3168282 0.6397636</span>
<span class="co">#&gt;  [743] 0.2570834 0.7746227 0.3168282 0.7746227 0.1729661 0.7746227 0.7746227</span>
<span class="co">#&gt;  [750] 0.7269985 0.1896267 0.7746227 0.7746227 0.2597384 0.7746227 0.1896267</span>
<span class="co">#&gt;  [757] 0.7544128 0.6402906 0.7746227 0.1896267 0.2570834 0.6397636 0.2570834</span>
<span class="co">#&gt;  [764] 0.7746227 0.7269985 0.7269985 0.3416221 0.6134946 0.1896267 0.6397636</span>
<span class="co">#&gt;  [771] 0.7746227 0.6397636 0.2570834 0.6397636 0.3168282 0.7544128 0.1896267</span>
<span class="co">#&gt;  [778] 0.3416221 0.2570834 0.7746227 0.2570834 0.7746227 0.3416221 0.6397636</span>
<span class="co">#&gt;  [785] 0.7746227 0.6397636 0.2597384 0.4791031 0.7746227 0.6397636 0.6134946</span>
<span class="co">#&gt;  [792] 0.1729661 0.7746227 0.7544128 0.7746227 0.6397636 0.7746227 0.6397636</span>
<span class="co">#&gt;  [799] 0.7746227 0.6397636 0.7746227 0.2570834 0.1896267 0.6134946 0.7746227</span>
<span class="co">#&gt;  [806] 0.6397636 0.1519797 0.2570834 0.1080889 0.7746227 0.2570834 0.2570834</span>
<span class="co">#&gt;  [813] 0.2570834 0.7269985 0.1896267 0.3416221 0.7269985 0.2570834 0.6402906</span>
<span class="co">#&gt;  [820] 0.7274523 0.1729661 0.7746227 0.6397636 0.7746227 0.6134946 0.8374954</span>
<span class="co">#&gt;  [827] 0.6397636 0.2570834 0.7746227 0.3168282 0.2570834 0.7544128 0.7746227</span>
<span class="co">#&gt;  [834] 0.3416221 0.6397636 0.2570834 0.2387325 0.6397636 0.7746227 0.6397636</span>
<span class="co">#&gt;  [841] 0.4791031 0.7746227 0.6397636 0.2362237 0.6134946 0.6397636 0.1896267</span>
<span class="co">#&gt;  [848] 0.8374954 0.7746227 0.1729661 0.6397636 0.7746227 0.6134946 0.7269985</span>
<span class="co">#&gt;  [855] 0.7746227 0.7269985 0.7746227 0.1080889 0.7544128 0.1729661 0.7544128</span>
<span class="co">#&gt;  [862] 0.8374954 0.7746227 0.7746227 0.6397636 0.7746227 0.8374954 0.7269985</span>
<span class="co">#&gt;  [869] 0.3416221 0.7746227 0.1896267 0.7746227 0.6134946 0.3168282 0.6134946</span>
<span class="co">#&gt;  [876] 0.7746227 0.2570834 0.6397636 0.7269985 0.2387325 0.1896267 0.2570834</span>
<span class="co">#&gt;  [883] 0.1519797 0.1519797 0.6397636 0.6134946 0.2362237 0.8374954 0.2570834</span>
<span class="co">#&gt;  [890] 0.7746227 0.7746227 0.2362237 0.2362237 0.8216251 0.3416221 0.6134946</span>
<span class="co">#&gt;  [897] 0.6134946 0.2362237 0.6397636 0.7041491 0.2570834 0.1729661 0.1896267</span>
<span class="co">#&gt;  [904] 0.3168282 0.6134946 0.2597384 0.7746227 0.7746227 0.7746227 0.6397636</span>
<span class="co">#&gt;  [911] 0.7746227 0.1896267 0.2362237 0.2597384 0.7746227 0.1896267 0.6397636</span>
<span class="co">#&gt;  [918] 0.7746227 0.2570834 0.6397636 0.7746227 0.7746227 0.2362237 0.6397636</span>
<span class="co">#&gt;  [925] 0.2570834 0.8374954 0.2362237 0.7746227 0.2570834 0.6134946 0.7746227</span>
<span class="co">#&gt;  [932] 0.7544128 0.1896267 0.6134946 0.7746227 0.6397636 0.1519797 0.7544128</span>
<span class="co">#&gt;  [939] 0.8374954 0.3416221 0.2570834 0.6402906 0.7544128 0.8374954 0.7746227</span>
<span class="co">#&gt;  [946] 0.7544128 0.2362237 0.2362237 0.6134946 0.6397636 0.6397636 0.6134946</span>
<span class="co">#&gt;  [953] 0.6397636 0.7544128 0.1896267 0.7746227 0.8374954 0.8374954 0.7544128</span>
<span class="co">#&gt;  [960] 0.2570834 0.2570834 0.7274523 0.2387325 0.7544128 0.2597384 0.7746227</span>
<span class="co">#&gt;  [967] 0.2597384 0.1896267 0.7746227 0.7269985 0.6134946 0.2570834 0.6397636</span>
<span class="co">#&gt;  [974] 0.7746227 0.6397636 0.7269985 0.1896267 0.1896267 0.6397636 0.8374954</span>
<span class="co">#&gt;  [981] 0.7041491 0.7269985 0.1729661 0.8374954 0.2570834 0.6397636 0.7746227</span>
<span class="co">#&gt;  [988] 0.6397636 0.6397636 0.8374954 0.2570834 0.2362237 0.6134946 0.8374954</span>
<span class="co">#&gt;  [995] 0.1729661 0.8374954 0.6134946 0.2362237 0.7746227 0.7544128</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; $linear.predictors</span>
<span class="co">#&gt;    [1] -1.56475023  1.23460032  0.97944724  0.97944724  0.46202560  1.63970937</span>
<span class="co">#&gt;    [7]  0.86713465  0.46202560 -1.45243763 -1.56475023 -0.08363644  1.52739678</span>
<span class="co">#&gt;   [13] -1.06118299  0.97944724  0.46202560  1.23460032  1.23460032 -0.76838654</span>
<span class="co">#&gt;   [19] -1.45243763 -1.17349559  0.97944724 -1.56475023  0.46202560  1.12228773</span>
<span class="co">#&gt;   [25]  0.57433819  1.23460032  1.23460032 -1.56475023  0.57433819  0.46202560</span>
<span class="co">#&gt;   [31] -1.56475023  1.23460032  1.63970937  1.12228773  0.86713465 -1.31404858</span>
<span class="co">#&gt;   [37]  0.57433819  0.57433819  0.98173473  0.46202560 -1.45243763 -1.45243763</span>
<span class="co">#&gt;   [43]  1.23460032  0.46202560  0.57433819  0.57433819  1.52739678 -0.65607394</span>
<span class="co">#&gt;   [49]  0.57433819 -1.06118299  0.57433819 -1.45243763  1.12228773  0.57433819</span>
<span class="co">#&gt;   [55]  1.23460032  1.23460032 -1.06118299  0.57433819 -1.17349559  1.23460032</span>
<span class="co">#&gt;   [61] -1.45243763  0.46202560 -1.45243763 -1.04732858 -0.65607394 -1.71915763</span>
<span class="co">#&gt;   [67]  1.63970937  1.12228773  1.63970937 -1.06118299 -1.56475023  0.57433819</span>
<span class="co">#&gt;   [73]  0.97944724 -1.06118299 -1.45243763 -1.06118299  1.63970937 -1.04732858</span>
<span class="co">#&gt;   [79]  1.23460032 -1.45243763  1.12228773 -1.17349559  0.97944724 -1.56475023</span>
<span class="co">#&gt;   [85]  0.57433819  1.23460032  1.23460032 -1.06118299  1.23460032 -1.06118299</span>
<span class="co">#&gt;   [91] -1.06118299  1.12228773 -1.56475023 -1.45243763  1.63970937  1.23460032</span>
<span class="co">#&gt;   [97]  0.97944724  1.12228773 -1.06118299  1.63970937 -1.06118299  1.23460032</span>
<span class="co">#&gt;  [103]  0.97944724  0.46202560  1.23460032 -0.65607394 -1.06118299  1.23460032</span>
<span class="co">#&gt;  [109] -1.06118299 -1.04732858  1.23460032  0.46202560 -1.06118299  1.23460032</span>
<span class="co">#&gt;  [115]  0.57433819  0.97944724  0.46202560  1.23460032  1.23460032 -1.04732858</span>
<span class="co">#&gt;  [121]  1.23460032  1.23460032  0.46202560 -1.45243763  1.12228773  0.46202560</span>
<span class="co">#&gt;  [127]  1.12228773  0.86713465 -1.56475023 -1.06118299  1.23460032 -1.45243763</span>
<span class="co">#&gt;  [133]  1.23460032  1.52739678 -1.17349559  1.23460032 -1.06118299  1.63970937</span>
<span class="co">#&gt;  [139]  1.23460032 -1.06118299  1.23460032  0.46202560  1.23460032 -2.11041227</span>
<span class="co">#&gt;  [145] -1.06118299  1.23460032 -2.11041227 -1.56475023 -1.56475023 -1.06118299</span>
<span class="co">#&gt;  [151]  1.12228773  1.52739678  1.23460032  0.46202560 -1.06118299  1.23460032</span>
<span class="co">#&gt;  [157] -1.45243763  0.46202560  0.46202560  0.46202560  1.12228773 -0.65607394</span>
<span class="co">#&gt;  [163] -1.56475023  0.57433819 -1.06118299  1.52739678 -1.06118299  0.57433819</span>
<span class="co">#&gt;  [169]  0.57433819  0.57433819  0.46202560  0.46202560  0.57433819  1.23460032</span>
<span class="co">#&gt;  [175]  0.97944724  0.46202560  1.52739678  0.46202560 -1.45243763 -1.04732858</span>
<span class="co">#&gt;  [181] -1.45243763  0.57433819 -1.06118299 -2.11041227  1.63970937  1.63970937</span>
<span class="co">#&gt;  [187] -1.56475023  1.63970937 -1.45243763  0.57433819 -1.17349559 -0.76838654</span>
<span class="co">#&gt;  [193]  1.23460032  1.23460032 -1.45243763 -0.65607394  1.12228773  0.57433819</span>
<span class="co">#&gt;  [199] -1.06118299  1.23460032  1.63970937  1.63970937  1.23460032  1.52739678</span>
<span class="co">#&gt;  [205]  0.57433819  0.57433819  1.52739678 -1.17349559  1.12228773 -1.06118299</span>
<span class="co">#&gt;  [211]  0.46202560  0.57433819 -1.06118299  0.57433819  1.23460032 -1.06118299</span>
<span class="co">#&gt;  [217] -1.06118299  1.12228773 -1.15964118  1.12228773 -1.45243763  1.23460032</span>
<span class="co">#&gt;  [223] -1.15964118 -1.45243763 -0.65607394 -1.45243763 -1.17349559  0.46202560</span>
<span class="co">#&gt;  [229]  1.23460032  1.23460032 -1.06118299  1.12228773 -1.71915763  1.63970937</span>
<span class="co">#&gt;  [235]  1.63970937 -1.15964118  0.57433819  1.23460032  0.57433819  0.46202560</span>
<span class="co">#&gt;  [241]  1.12228773  0.57433819 -1.17349559 -1.06118299  0.46202560  1.23460032</span>
<span class="co">#&gt;  [247] -1.45243763 -0.08363644  0.57433819  1.23460032  0.57433819  1.23460032</span>
<span class="co">#&gt;  [253]  1.23460032  0.97944724  0.57433819  1.12228773  1.23460032  1.63970937</span>
<span class="co">#&gt;  [259]  1.12228773  0.57433819 -1.56475023  1.12228773  0.57433819 -1.45243763</span>
<span class="co">#&gt;  [265]  1.12228773 -1.06118299 -0.08363644  1.23460032  0.57433819 -1.04732858</span>
<span class="co">#&gt;  [271]  1.23460032  1.23460032  1.52739678  1.23460032  1.12228773 -1.06118299</span>
<span class="co">#&gt;  [277] -1.17349559  1.63970937  0.46202560  0.46202560  0.57433819 -1.06118299</span>
<span class="co">#&gt;  [283] -1.04732858  0.57433819  1.12228773  1.23460032  1.12228773 -0.76838654</span>
<span class="co">#&gt;  [289]  0.57433819  0.46202560 -0.08363644  0.57433819  1.23460032  0.46202560</span>
<span class="co">#&gt;  [295]  1.23460032 -1.06118299  0.46202560  1.23460032  1.63970937 -1.56475023</span>
<span class="co">#&gt;  [301]  1.23460032  1.12228773  0.57433819 -1.45243763  1.23460032  0.97944724</span>
<span class="co">#&gt;  [307]  1.23460032  1.63970937  1.23460032 -1.17349559 -1.06118299 -1.06118299</span>
<span class="co">#&gt;  [313] -1.06118299  0.46202560  1.23460032  0.46202560  0.97944724  0.46202560</span>
<span class="co">#&gt;  [319]  1.52739678  0.57433819  1.12228773  1.23460032  1.63970937  0.46202560</span>
<span class="co">#&gt;  [325]  0.57433819 -1.31404858  1.63970937  0.57433819 -1.06118299 -0.76838654</span>
<span class="co">#&gt;  [331] -0.65607394  1.12228773 -0.65607394  1.23460032 -1.04732858  1.12228773</span>
<span class="co">#&gt;  [337]  1.12228773  0.46202560  0.57433819 -1.56475023  1.12228773  1.23460032</span>
<span class="co">#&gt;  [343] -1.04732858 -1.04732858  1.23460032 -1.17349559  0.86713465  1.23460032</span>
<span class="co">#&gt;  [349] -1.31404858 -1.17349559  0.57433819  0.46202560  1.63970937  0.46202560</span>
<span class="co">#&gt;  [355]  1.23460032  1.23460032 -1.45243763  0.46202560 -1.45243763  1.63970937</span>
<span class="co">#&gt;  [361]  1.52739678 -1.45243763  0.46202560  1.12228773  0.46202560 -1.17349559</span>
<span class="co">#&gt;  [367]  1.63970937 -1.45243763 -1.06118299 -1.45243763 -0.65607394  0.46202560</span>
<span class="co">#&gt;  [373] -1.06118299  0.57433819  1.52739678 -1.06118299  1.12228773  0.97944724</span>
<span class="co">#&gt;  [379]  1.12228773  0.97944724 -1.06118299  0.57433819 -1.06118299  1.12228773</span>
<span class="co">#&gt;  [385] -1.56475023  0.46202560 -1.56475023  1.23460032 -1.45243763  1.23460032</span>
<span class="co">#&gt;  [391] -2.11041227  0.57433819  1.23460032  0.97944724  0.46202560 -1.06118299</span>
<span class="co">#&gt;  [397]  1.12228773  0.97944724 -1.45243763  0.97944724 -1.45243763  1.63970937</span>
<span class="co">#&gt;  [403]  1.23460032  0.97944724 -1.06118299 -0.65607394  1.12228773  1.23460032</span>
<span class="co">#&gt;  [409] -1.06118299 -0.08363644  0.46202560 -1.56475023 -1.45243763  0.57433819</span>
<span class="co">#&gt;  [415]  0.46202560 -1.06118299  1.52739678  1.23460032  1.23460032 -0.65607394</span>
<span class="co">#&gt;  [421]  1.52739678 -1.45243763 -1.17349559  1.12228773  0.46202560  1.12228773</span>
<span class="co">#&gt;  [427] -1.56475023 -0.65607394  0.57433819 -1.45243763  0.46202560 -1.06118299</span>
<span class="co">#&gt;  [433] -1.17349559 -0.76838654  0.57433819 -1.06118299  1.23460032 -1.06118299</span>
<span class="co">#&gt;  [439]  0.57433819  0.97944724  1.23460032  0.46202560  1.12228773 -1.06118299</span>
<span class="co">#&gt;  [445] -1.06118299  1.23460032  1.23460032  0.57433819  1.23460032 -1.17349559</span>
<span class="co">#&gt;  [451]  0.57433819  1.23460032 -1.56475023  0.46202560  1.23460032  0.46202560</span>
<span class="co">#&gt;  [457]  0.57433819  0.46202560  1.12228773  1.12228773  1.23460032 -1.45243763</span>
<span class="co">#&gt;  [463]  1.23460032 -1.06118299 -1.56475023 -0.76838654 -1.06118299  0.46202560</span>
<span class="co">#&gt;  [469]  1.23460032  1.23460032  1.63970937 -0.76838654 -1.17349559  0.57433819</span>
<span class="co">#&gt;  [475] -1.06118299  0.57433819  1.63970937  0.57433819  0.46202560 -1.04732858</span>
<span class="co">#&gt;  [481] -0.65607394 -1.45243763  1.63970937  1.63970937  0.46202560  1.52739678</span>
<span class="co">#&gt;  [487] -1.06118299  1.63970937  1.23460032  0.46202560 -1.45243763  0.57433819</span>
<span class="co">#&gt;  [493]  0.57662568 -0.65607394  1.23460032  0.57433819 -1.45243763  1.12228773</span>
<span class="co">#&gt;  [499]  1.23460032 -1.06118299  0.46202560  1.63970937 -0.65607394 -1.45243763</span>
<span class="co">#&gt;  [505] -1.06118299 -1.17349559  0.57433819  1.12228773  0.57433819  1.23460032</span>
<span class="co">#&gt;  [511]  1.12228773 -1.06118299 -1.56475023  0.46202560  0.57433819  1.12228773</span>
<span class="co">#&gt;  [517]  0.57433819  1.23460032  0.57433819  1.52739678  0.46202560  1.23460032</span>
<span class="co">#&gt;  [523]  0.46202560  0.57433819 -0.08363644 -1.17349559  0.57433819  0.46202560</span>
<span class="co">#&gt;  [529] -1.17349559  1.23460032  1.23460032  1.52739678  1.23460032 -1.06118299</span>
<span class="co">#&gt;  [535]  1.63970937  0.46202560  0.46202560  0.86713465  1.23460032 -1.17349559</span>
<span class="co">#&gt;  [541]  0.97944724  1.23460032  1.23460032  1.23460032 -1.06118299 -1.06118299</span>
<span class="co">#&gt;  [547] -0.76838654  0.57433819  1.23460032  1.63970937  0.97944724 -0.76838654</span>
<span class="co">#&gt;  [553]  1.23460032  0.97944724 -1.06118299  0.57433819  1.12228773 -1.06118299</span>
<span class="co">#&gt;  [559]  0.57433819 -0.76838654  0.57433819  0.57433819  1.23460032 -1.04732858</span>
<span class="co">#&gt;  [565]  0.57433819  0.57433819  1.23460032  1.52739678  1.63970937 -0.65607394</span>
<span class="co">#&gt;  [571] -0.08363644 -1.04732858  0.57433819 -1.45243763 -1.17349559 -1.56475023</span>
<span class="co">#&gt;  [577] -1.06118299  0.57433819  1.23460032  0.57433819 -1.06118299  1.12228773</span>
<span class="co">#&gt;  [583]  0.97944724 -1.04732858  0.57433819  0.57433819  1.52739678  1.63970937</span>
<span class="co">#&gt;  [589]  1.23460032  0.57433819  1.12228773  0.97944724  0.57433819 -1.04732858</span>
<span class="co">#&gt;  [595]  0.46202560  0.97944724  1.12228773  1.23460032 -1.06118299 -1.06118299</span>
<span class="co">#&gt;  [601]  0.57433819  0.97944724  1.12228773 -1.45243763 -1.17349559  1.52739678</span>
<span class="co">#&gt;  [607]  0.97944724  1.23460032  1.23460032  0.86713465 -0.65607394  0.57433819</span>
<span class="co">#&gt;  [613] -1.06118299  0.57433819  1.12228773  0.57433819  1.12228773  0.46202560</span>
<span class="co">#&gt;  [619]  0.46202560  0.46202560  1.12228773  0.46202560 -1.15964118  1.23460032</span>
<span class="co">#&gt;  [625] -0.65607394  1.23460032 -1.17349559 -1.45243763  1.23460032  1.23460032</span>
<span class="co">#&gt;  [631]  1.23460032  0.57433819 -0.08363644 -1.06118299 -1.06118299  0.86713465</span>
<span class="co">#&gt;  [637] -1.56475023  0.46202560  1.23460032  0.57433819  1.23460032  1.12228773</span>
<span class="co">#&gt;  [643]  0.57433819  1.12228773 -1.17349559  1.23460032 -1.06118299 -0.08363644</span>
<span class="co">#&gt;  [649]  1.63970937 -1.04732858 -1.06118299 -1.06118299  0.57433819  1.12228773</span>
<span class="co">#&gt;  [655] -0.65607394  1.12228773  0.57433819  1.23460032  1.23460032 -1.06118299</span>
<span class="co">#&gt;  [661] -1.06118299  1.63970937 -1.06118299 -1.04732858 -1.17349559  0.46202560</span>
<span class="co">#&gt;  [667]  0.57662568  0.57433819 -0.76838654  1.12228773 -1.17349559  1.12228773</span>
<span class="co">#&gt;  [673] -1.17349559  0.86713465  0.57433819  0.57433819  1.23460032  0.57433819</span>
<span class="co">#&gt;  [679]  1.23460032  0.57433819  0.46202560 -0.65607394  1.12228773  1.63970937</span>
<span class="co">#&gt;  [685]  1.52739678  1.23460032  0.57433819  0.57433819  0.57433819  1.12228773</span>
<span class="co">#&gt;  [691] -1.17349559  0.57433819  1.23460032  1.23460032  0.86713465  0.46202560</span>
<span class="co">#&gt;  [697]  0.46202560  0.97944724 -0.76838654 -1.45243763 -1.06118299  0.97944724</span>
<span class="co">#&gt;  [703]  1.23460032  1.12228773 -1.17349559  1.23460032  1.23460032  1.12228773</span>
<span class="co">#&gt;  [709]  0.57433819 -1.71915763  0.57433819  1.12228773  1.52739678 -1.56475023</span>
<span class="co">#&gt;  [715] -1.06118299  0.97944724  1.63970937  0.57433819  0.57433819  0.57433819</span>
<span class="co">#&gt;  [721]  0.46202560  1.23460032 -1.06118299  1.23460032 -1.06118299  0.97944724</span>
<span class="co">#&gt;  [727]  0.57433819  0.46202560  1.12228773  0.57433819 -1.06118299  1.63970937</span>
<span class="co">#&gt;  [733] -1.04732858 -1.06118299  0.46202560  0.46202560  0.46202560  1.12228773</span>
<span class="co">#&gt;  [739] -0.76838654 -1.06118299 -0.76838654  0.57433819 -1.06118299  1.23460032</span>
<span class="co">#&gt;  [745] -0.76838654  1.23460032 -1.56475023  1.23460032  1.23460032  0.97944724</span>
<span class="co">#&gt;  [751] -1.45243763  1.23460032  1.23460032 -1.04732858  1.23460032 -1.45243763</span>
<span class="co">#&gt;  [757]  1.12228773  0.57662568  1.23460032 -1.45243763 -1.06118299  0.57433819</span>
<span class="co">#&gt;  [763] -1.06118299  1.23460032  0.97944724  0.97944724 -0.65607394  0.46202560</span>
<span class="co">#&gt;  [769] -1.45243763  0.57433819  1.23460032  0.57433819 -1.06118299  0.57433819</span>
<span class="co">#&gt;  [775] -0.76838654  1.12228773 -1.45243763 -0.65607394 -1.06118299  1.23460032</span>
<span class="co">#&gt;  [781] -1.06118299  1.23460032 -0.65607394  0.57433819  1.23460032  0.57433819</span>
<span class="co">#&gt;  [787] -1.04732858 -0.08363644  1.23460032  0.57433819  0.46202560 -1.56475023</span>
<span class="co">#&gt;  [793]  1.23460032  1.12228773  1.23460032  0.57433819  1.23460032  0.57433819</span>
<span class="co">#&gt;  [799]  1.23460032  0.57433819  1.23460032 -1.06118299 -1.45243763  0.46202560</span>
<span class="co">#&gt;  [805]  1.23460032  0.57433819 -1.71915763 -1.06118299 -2.11041227  1.23460032</span>
<span class="co">#&gt;  [811] -1.06118299 -1.06118299 -1.06118299  0.97944724 -1.45243763 -0.65607394</span>
<span class="co">#&gt;  [817]  0.97944724 -1.06118299  0.57662568  0.98173473 -1.56475023  1.23460032</span>
<span class="co">#&gt;  [823]  0.57433819  1.23460032  0.46202560  1.63970937  0.57433819 -1.06118299</span>
<span class="co">#&gt;  [829]  1.23460032 -0.76838654 -1.06118299  1.12228773  1.23460032 -0.65607394</span>
<span class="co">#&gt;  [835]  0.57433819 -1.06118299 -1.15964118  0.57433819  1.23460032  0.57433819</span>
<span class="co">#&gt;  [841] -0.08363644  1.23460032  0.57433819 -1.17349559  0.46202560  0.57433819</span>
<span class="co">#&gt;  [847] -1.45243763  1.63970937  1.23460032 -1.56475023  0.57433819  1.23460032</span>
<span class="co">#&gt;  [853]  0.46202560  0.97944724  1.23460032  0.97944724  1.23460032 -2.11041227</span>
<span class="co">#&gt;  [859]  1.12228773 -1.56475023  1.12228773  1.63970937  1.23460032  1.23460032</span>
<span class="co">#&gt;  [865]  0.57433819  1.23460032  1.63970937  0.97944724 -0.65607394  1.23460032</span>
<span class="co">#&gt;  [871] -1.45243763  1.23460032  0.46202560 -0.76838654  0.46202560  1.23460032</span>
<span class="co">#&gt;  [877] -1.06118299  0.57433819  0.97944724 -1.15964118 -1.45243763 -1.06118299</span>
<span class="co">#&gt;  [883] -1.71915763 -1.71915763  0.57433819  0.46202560 -1.17349559  1.63970937</span>
<span class="co">#&gt;  [889] -1.06118299  1.23460032  1.23460032 -1.17349559 -1.17349559  1.52739678</span>
<span class="co">#&gt;  [895] -0.65607394  0.46202560  0.46202560 -1.17349559  0.57433819  0.86713465</span>
<span class="co">#&gt;  [901] -1.06118299 -1.56475023 -1.45243763 -0.76838654  0.46202560 -1.04732858</span>
<span class="co">#&gt;  [907]  1.23460032  1.23460032  1.23460032  0.57433819  1.23460032 -1.45243763</span>
<span class="co">#&gt;  [913] -1.17349559 -1.04732858  1.23460032 -1.45243763  0.57433819  1.23460032</span>
<span class="co">#&gt;  [919] -1.06118299  0.57433819  1.23460032  1.23460032 -1.17349559  0.57433819</span>
<span class="co">#&gt;  [925] -1.06118299  1.63970937 -1.17349559  1.23460032 -1.06118299  0.46202560</span>
<span class="co">#&gt;  [931]  1.23460032  1.12228773 -1.45243763  0.46202560  1.23460032  0.57433819</span>
<span class="co">#&gt;  [937] -1.71915763  1.12228773  1.63970937 -0.65607394 -1.06118299  0.57662568</span>
<span class="co">#&gt;  [943]  1.12228773  1.63970937  1.23460032  1.12228773 -1.17349559 -1.17349559</span>
<span class="co">#&gt;  [949]  0.46202560  0.57433819  0.57433819  0.46202560  0.57433819  1.12228773</span>
<span class="co">#&gt;  [955] -1.45243763  1.23460032  1.63970937  1.63970937  1.12228773 -1.06118299</span>
<span class="co">#&gt;  [961] -1.06118299  0.98173473 -1.15964118  1.12228773 -1.04732858  1.23460032</span>
<span class="co">#&gt;  [967] -1.04732858 -1.45243763  1.23460032  0.97944724  0.46202560 -1.06118299</span>
<span class="co">#&gt;  [973]  0.57433819  1.23460032  0.57433819  0.97944724 -1.45243763 -1.45243763</span>
<span class="co">#&gt;  [979]  0.57433819  1.63970937  0.86713465  0.97944724 -1.56475023  1.63970937</span>
<span class="co">#&gt;  [985] -1.06118299  0.57433819  1.23460032  0.57433819  0.57433819  1.63970937</span>
<span class="co">#&gt;  [991] -1.06118299 -1.17349559  0.46202560  1.63970937 -1.56475023  1.63970937</span>
<span class="co">#&gt;  [997]  0.46202560 -1.17349559  1.23460032  1.12228773</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; $loglikelihood</span>
<span class="co">#&gt; [1] -567.9048</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; $converged</span>
<span class="co">#&gt; [1] TRUE</span></pre></body></html></div>
<p>Its S4 equivalent <code>show</code> is also available:</p>
<div class="sourceCode" id="cb15"><html><body><pre class="r"><span class="fu">show</span>(<span class="no">discretization</span>)
<span class="co">#&gt; $coefficients</span>
<span class="co">#&gt; [1] -1.32943135  1.35760660 -0.93817671  0.69734447 -0.30888302  0.09622603</span>
<span class="co">#&gt; [7]  0.18587674 -0.47209789  0.07356415</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; $fitted.values</span>
<span class="co">#&gt;    [1] 0.1729661 0.7746227 0.7269985 0.7269985 0.6134946 0.8374954 0.7041491</span>
<span class="co">#&gt;    [8] 0.6134946 0.1896267 0.1729661 0.4791031 0.8216251 0.2570834 0.7269985</span>
<span class="co">#&gt;   [15] 0.6134946 0.7746227 0.7746227 0.3168282 0.1896267 0.2362237 0.7269985</span>
<span class="co">#&gt;   [22] 0.1729661 0.6134946 0.7544128 0.6397636 0.7746227 0.7746227 0.1729661</span>
<span class="co">#&gt;   [29] 0.6397636 0.6134946 0.1729661 0.7746227 0.8374954 0.7544128 0.7041491</span>
<span class="co">#&gt;   [36] 0.2118102 0.6397636 0.6397636 0.7274523 0.6134946 0.1896267 0.1896267</span>
<span class="co">#&gt;   [43] 0.7746227 0.6134946 0.6397636 0.6397636 0.8216251 0.3416221 0.6397636</span>
<span class="co">#&gt;   [50] 0.2570834 0.6397636 0.1896267 0.7544128 0.6397636 0.7746227 0.7746227</span>
<span class="co">#&gt;   [57] 0.2570834 0.6397636 0.2362237 0.7746227 0.1896267 0.6134946 0.1896267</span>
<span class="co">#&gt;   [64] 0.2597384 0.3416221 0.1519797 0.8374954 0.7544128 0.8374954 0.2570834</span>
<span class="co">#&gt;   [71] 0.1729661 0.6397636 0.7269985 0.2570834 0.1896267 0.2570834 0.8374954</span>
<span class="co">#&gt;   [78] 0.2597384 0.7746227 0.1896267 0.7544128 0.2362237 0.7269985 0.1729661</span>
<span class="co">#&gt;   [85] 0.6397636 0.7746227 0.7746227 0.2570834 0.7746227 0.2570834 0.2570834</span>
<span class="co">#&gt;   [92] 0.7544128 0.1729661 0.1896267 0.8374954 0.7746227 0.7269985 0.7544128</span>
<span class="co">#&gt;   [99] 0.2570834 0.8374954 0.2570834 0.7746227 0.7269985 0.6134946 0.7746227</span>
<span class="co">#&gt;  [106] 0.3416221 0.2570834 0.7746227 0.2570834 0.2597384 0.7746227 0.6134946</span>
<span class="co">#&gt;  [113] 0.2570834 0.7746227 0.6397636 0.7269985 0.6134946 0.7746227 0.7746227</span>
<span class="co">#&gt;  [120] 0.2597384 0.7746227 0.7746227 0.6134946 0.1896267 0.7544128 0.6134946</span>
<span class="co">#&gt;  [127] 0.7544128 0.7041491 0.1729661 0.2570834 0.7746227 0.1896267 0.7746227</span>
<span class="co">#&gt;  [134] 0.8216251 0.2362237 0.7746227 0.2570834 0.8374954 0.7746227 0.2570834</span>
<span class="co">#&gt;  [141] 0.7746227 0.6134946 0.7746227 0.1080889 0.2570834 0.7746227 0.1080889</span>
<span class="co">#&gt;  [148] 0.1729661 0.1729661 0.2570834 0.7544128 0.8216251 0.7746227 0.6134946</span>
<span class="co">#&gt;  [155] 0.2570834 0.7746227 0.1896267 0.6134946 0.6134946 0.6134946 0.7544128</span>
<span class="co">#&gt;  [162] 0.3416221 0.1729661 0.6397636 0.2570834 0.8216251 0.2570834 0.6397636</span>
<span class="co">#&gt;  [169] 0.6397636 0.6397636 0.6134946 0.6134946 0.6397636 0.7746227 0.7269985</span>
<span class="co">#&gt;  [176] 0.6134946 0.8216251 0.6134946 0.1896267 0.2597384 0.1896267 0.6397636</span>
<span class="co">#&gt;  [183] 0.2570834 0.1080889 0.8374954 0.8374954 0.1729661 0.8374954 0.1896267</span>
<span class="co">#&gt;  [190] 0.6397636 0.2362237 0.3168282 0.7746227 0.7746227 0.1896267 0.3416221</span>
<span class="co">#&gt;  [197] 0.7544128 0.6397636 0.2570834 0.7746227 0.8374954 0.8374954 0.7746227</span>
<span class="co">#&gt;  [204] 0.8216251 0.6397636 0.6397636 0.8216251 0.2362237 0.7544128 0.2570834</span>
<span class="co">#&gt;  [211] 0.6134946 0.6397636 0.2570834 0.6397636 0.7746227 0.2570834 0.2570834</span>
<span class="co">#&gt;  [218] 0.7544128 0.2387325 0.7544128 0.1896267 0.7746227 0.2387325 0.1896267</span>
<span class="co">#&gt;  [225] 0.3416221 0.1896267 0.2362237 0.6134946 0.7746227 0.7746227 0.2570834</span>
<span class="co">#&gt;  [232] 0.7544128 0.1519797 0.8374954 0.8374954 0.2387325 0.6397636 0.7746227</span>
<span class="co">#&gt;  [239] 0.6397636 0.6134946 0.7544128 0.6397636 0.2362237 0.2570834 0.6134946</span>
<span class="co">#&gt;  [246] 0.7746227 0.1896267 0.4791031 0.6397636 0.7746227 0.6397636 0.7746227</span>
<span class="co">#&gt;  [253] 0.7746227 0.7269985 0.6397636 0.7544128 0.7746227 0.8374954 0.7544128</span>
<span class="co">#&gt;  [260] 0.6397636 0.1729661 0.7544128 0.6397636 0.1896267 0.7544128 0.2570834</span>
<span class="co">#&gt;  [267] 0.4791031 0.7746227 0.6397636 0.2597384 0.7746227 0.7746227 0.8216251</span>
<span class="co">#&gt;  [274] 0.7746227 0.7544128 0.2570834 0.2362237 0.8374954 0.6134946 0.6134946</span>
<span class="co">#&gt;  [281] 0.6397636 0.2570834 0.2597384 0.6397636 0.7544128 0.7746227 0.7544128</span>
<span class="co">#&gt;  [288] 0.3168282 0.6397636 0.6134946 0.4791031 0.6397636 0.7746227 0.6134946</span>
<span class="co">#&gt;  [295] 0.7746227 0.2570834 0.6134946 0.7746227 0.8374954 0.1729661 0.7746227</span>
<span class="co">#&gt;  [302] 0.7544128 0.6397636 0.1896267 0.7746227 0.7269985 0.7746227 0.8374954</span>
<span class="co">#&gt;  [309] 0.7746227 0.2362237 0.2570834 0.2570834 0.2570834 0.6134946 0.7746227</span>
<span class="co">#&gt;  [316] 0.6134946 0.7269985 0.6134946 0.8216251 0.6397636 0.7544128 0.7746227</span>
<span class="co">#&gt;  [323] 0.8374954 0.6134946 0.6397636 0.2118102 0.8374954 0.6397636 0.2570834</span>
<span class="co">#&gt;  [330] 0.3168282 0.3416221 0.7544128 0.3416221 0.7746227 0.2597384 0.7544128</span>
<span class="co">#&gt;  [337] 0.7544128 0.6134946 0.6397636 0.1729661 0.7544128 0.7746227 0.2597384</span>
<span class="co">#&gt;  [344] 0.2597384 0.7746227 0.2362237 0.7041491 0.7746227 0.2118102 0.2362237</span>
<span class="co">#&gt;  [351] 0.6397636 0.6134946 0.8374954 0.6134946 0.7746227 0.7746227 0.1896267</span>
<span class="co">#&gt;  [358] 0.6134946 0.1896267 0.8374954 0.8216251 0.1896267 0.6134946 0.7544128</span>
<span class="co">#&gt;  [365] 0.6134946 0.2362237 0.8374954 0.1896267 0.2570834 0.1896267 0.3416221</span>
<span class="co">#&gt;  [372] 0.6134946 0.2570834 0.6397636 0.8216251 0.2570834 0.7544128 0.7269985</span>
<span class="co">#&gt;  [379] 0.7544128 0.7269985 0.2570834 0.6397636 0.2570834 0.7544128 0.1729661</span>
<span class="co">#&gt;  [386] 0.6134946 0.1729661 0.7746227 0.1896267 0.7746227 0.1080889 0.6397636</span>
<span class="co">#&gt;  [393] 0.7746227 0.7269985 0.6134946 0.2570834 0.7544128 0.7269985 0.1896267</span>
<span class="co">#&gt;  [400] 0.7269985 0.1896267 0.8374954 0.7746227 0.7269985 0.2570834 0.3416221</span>
<span class="co">#&gt;  [407] 0.7544128 0.7746227 0.2570834 0.4791031 0.6134946 0.1729661 0.1896267</span>
<span class="co">#&gt;  [414] 0.6397636 0.6134946 0.2570834 0.8216251 0.7746227 0.7746227 0.3416221</span>
<span class="co">#&gt;  [421] 0.8216251 0.1896267 0.2362237 0.7544128 0.6134946 0.7544128 0.1729661</span>
<span class="co">#&gt;  [428] 0.3416221 0.6397636 0.1896267 0.6134946 0.2570834 0.2362237 0.3168282</span>
<span class="co">#&gt;  [435] 0.6397636 0.2570834 0.7746227 0.2570834 0.6397636 0.7269985 0.7746227</span>
<span class="co">#&gt;  [442] 0.6134946 0.7544128 0.2570834 0.2570834 0.7746227 0.7746227 0.6397636</span>
<span class="co">#&gt;  [449] 0.7746227 0.2362237 0.6397636 0.7746227 0.1729661 0.6134946 0.7746227</span>
<span class="co">#&gt;  [456] 0.6134946 0.6397636 0.6134946 0.7544128 0.7544128 0.7746227 0.1896267</span>
<span class="co">#&gt;  [463] 0.7746227 0.2570834 0.1729661 0.3168282 0.2570834 0.6134946 0.7746227</span>
<span class="co">#&gt;  [470] 0.7746227 0.8374954 0.3168282 0.2362237 0.6397636 0.2570834 0.6397636</span>
<span class="co">#&gt;  [477] 0.8374954 0.6397636 0.6134946 0.2597384 0.3416221 0.1896267 0.8374954</span>
<span class="co">#&gt;  [484] 0.8374954 0.6134946 0.8216251 0.2570834 0.8374954 0.7746227 0.6134946</span>
<span class="co">#&gt;  [491] 0.1896267 0.6397636 0.6402906 0.3416221 0.7746227 0.6397636 0.1896267</span>
<span class="co">#&gt;  [498] 0.7544128 0.7746227 0.2570834 0.6134946 0.8374954 0.3416221 0.1896267</span>
<span class="co">#&gt;  [505] 0.2570834 0.2362237 0.6397636 0.7544128 0.6397636 0.7746227 0.7544128</span>
<span class="co">#&gt;  [512] 0.2570834 0.1729661 0.6134946 0.6397636 0.7544128 0.6397636 0.7746227</span>
<span class="co">#&gt;  [519] 0.6397636 0.8216251 0.6134946 0.7746227 0.6134946 0.6397636 0.4791031</span>
<span class="co">#&gt;  [526] 0.2362237 0.6397636 0.6134946 0.2362237 0.7746227 0.7746227 0.8216251</span>
<span class="co">#&gt;  [533] 0.7746227 0.2570834 0.8374954 0.6134946 0.6134946 0.7041491 0.7746227</span>
<span class="co">#&gt;  [540] 0.2362237 0.7269985 0.7746227 0.7746227 0.7746227 0.2570834 0.2570834</span>
<span class="co">#&gt;  [547] 0.3168282 0.6397636 0.7746227 0.8374954 0.7269985 0.3168282 0.7746227</span>
<span class="co">#&gt;  [554] 0.7269985 0.2570834 0.6397636 0.7544128 0.2570834 0.6397636 0.3168282</span>
<span class="co">#&gt;  [561] 0.6397636 0.6397636 0.7746227 0.2597384 0.6397636 0.6397636 0.7746227</span>
<span class="co">#&gt;  [568] 0.8216251 0.8374954 0.3416221 0.4791031 0.2597384 0.6397636 0.1896267</span>
<span class="co">#&gt;  [575] 0.2362237 0.1729661 0.2570834 0.6397636 0.7746227 0.6397636 0.2570834</span>
<span class="co">#&gt;  [582] 0.7544128 0.7269985 0.2597384 0.6397636 0.6397636 0.8216251 0.8374954</span>
<span class="co">#&gt;  [589] 0.7746227 0.6397636 0.7544128 0.7269985 0.6397636 0.2597384 0.6134946</span>
<span class="co">#&gt;  [596] 0.7269985 0.7544128 0.7746227 0.2570834 0.2570834 0.6397636 0.7269985</span>
<span class="co">#&gt;  [603] 0.7544128 0.1896267 0.2362237 0.8216251 0.7269985 0.7746227 0.7746227</span>
<span class="co">#&gt;  [610] 0.7041491 0.3416221 0.6397636 0.2570834 0.6397636 0.7544128 0.6397636</span>
<span class="co">#&gt;  [617] 0.7544128 0.6134946 0.6134946 0.6134946 0.7544128 0.6134946 0.2387325</span>
<span class="co">#&gt;  [624] 0.7746227 0.3416221 0.7746227 0.2362237 0.1896267 0.7746227 0.7746227</span>
<span class="co">#&gt;  [631] 0.7746227 0.6397636 0.4791031 0.2570834 0.2570834 0.7041491 0.1729661</span>
<span class="co">#&gt;  [638] 0.6134946 0.7746227 0.6397636 0.7746227 0.7544128 0.6397636 0.7544128</span>
<span class="co">#&gt;  [645] 0.2362237 0.7746227 0.2570834 0.4791031 0.8374954 0.2597384 0.2570834</span>
<span class="co">#&gt;  [652] 0.2570834 0.6397636 0.7544128 0.3416221 0.7544128 0.6397636 0.7746227</span>
<span class="co">#&gt;  [659] 0.7746227 0.2570834 0.2570834 0.8374954 0.2570834 0.2597384 0.2362237</span>
<span class="co">#&gt;  [666] 0.6134946 0.6402906 0.6397636 0.3168282 0.7544128 0.2362237 0.7544128</span>
<span class="co">#&gt;  [673] 0.2362237 0.7041491 0.6397636 0.6397636 0.7746227 0.6397636 0.7746227</span>
<span class="co">#&gt;  [680] 0.6397636 0.6134946 0.3416221 0.7544128 0.8374954 0.8216251 0.7746227</span>
<span class="co">#&gt;  [687] 0.6397636 0.6397636 0.6397636 0.7544128 0.2362237 0.6397636 0.7746227</span>
<span class="co">#&gt;  [694] 0.7746227 0.7041491 0.6134946 0.6134946 0.7269985 0.3168282 0.1896267</span>
<span class="co">#&gt;  [701] 0.2570834 0.7269985 0.7746227 0.7544128 0.2362237 0.7746227 0.7746227</span>
<span class="co">#&gt;  [708] 0.7544128 0.6397636 0.1519797 0.6397636 0.7544128 0.8216251 0.1729661</span>
<span class="co">#&gt;  [715] 0.2570834 0.7269985 0.8374954 0.6397636 0.6397636 0.6397636 0.6134946</span>
<span class="co">#&gt;  [722] 0.7746227 0.2570834 0.7746227 0.2570834 0.7269985 0.6397636 0.6134946</span>
<span class="co">#&gt;  [729] 0.7544128 0.6397636 0.2570834 0.8374954 0.2597384 0.2570834 0.6134946</span>
<span class="co">#&gt;  [736] 0.6134946 0.6134946 0.7544128 0.3168282 0.2570834 0.3168282 0.6397636</span>
<span class="co">#&gt;  [743] 0.2570834 0.7746227 0.3168282 0.7746227 0.1729661 0.7746227 0.7746227</span>
<span class="co">#&gt;  [750] 0.7269985 0.1896267 0.7746227 0.7746227 0.2597384 0.7746227 0.1896267</span>
<span class="co">#&gt;  [757] 0.7544128 0.6402906 0.7746227 0.1896267 0.2570834 0.6397636 0.2570834</span>
<span class="co">#&gt;  [764] 0.7746227 0.7269985 0.7269985 0.3416221 0.6134946 0.1896267 0.6397636</span>
<span class="co">#&gt;  [771] 0.7746227 0.6397636 0.2570834 0.6397636 0.3168282 0.7544128 0.1896267</span>
<span class="co">#&gt;  [778] 0.3416221 0.2570834 0.7746227 0.2570834 0.7746227 0.3416221 0.6397636</span>
<span class="co">#&gt;  [785] 0.7746227 0.6397636 0.2597384 0.4791031 0.7746227 0.6397636 0.6134946</span>
<span class="co">#&gt;  [792] 0.1729661 0.7746227 0.7544128 0.7746227 0.6397636 0.7746227 0.6397636</span>
<span class="co">#&gt;  [799] 0.7746227 0.6397636 0.7746227 0.2570834 0.1896267 0.6134946 0.7746227</span>
<span class="co">#&gt;  [806] 0.6397636 0.1519797 0.2570834 0.1080889 0.7746227 0.2570834 0.2570834</span>
<span class="co">#&gt;  [813] 0.2570834 0.7269985 0.1896267 0.3416221 0.7269985 0.2570834 0.6402906</span>
<span class="co">#&gt;  [820] 0.7274523 0.1729661 0.7746227 0.6397636 0.7746227 0.6134946 0.8374954</span>
<span class="co">#&gt;  [827] 0.6397636 0.2570834 0.7746227 0.3168282 0.2570834 0.7544128 0.7746227</span>
<span class="co">#&gt;  [834] 0.3416221 0.6397636 0.2570834 0.2387325 0.6397636 0.7746227 0.6397636</span>
<span class="co">#&gt;  [841] 0.4791031 0.7746227 0.6397636 0.2362237 0.6134946 0.6397636 0.1896267</span>
<span class="co">#&gt;  [848] 0.8374954 0.7746227 0.1729661 0.6397636 0.7746227 0.6134946 0.7269985</span>
<span class="co">#&gt;  [855] 0.7746227 0.7269985 0.7746227 0.1080889 0.7544128 0.1729661 0.7544128</span>
<span class="co">#&gt;  [862] 0.8374954 0.7746227 0.7746227 0.6397636 0.7746227 0.8374954 0.7269985</span>
<span class="co">#&gt;  [869] 0.3416221 0.7746227 0.1896267 0.7746227 0.6134946 0.3168282 0.6134946</span>
<span class="co">#&gt;  [876] 0.7746227 0.2570834 0.6397636 0.7269985 0.2387325 0.1896267 0.2570834</span>
<span class="co">#&gt;  [883] 0.1519797 0.1519797 0.6397636 0.6134946 0.2362237 0.8374954 0.2570834</span>
<span class="co">#&gt;  [890] 0.7746227 0.7746227 0.2362237 0.2362237 0.8216251 0.3416221 0.6134946</span>
<span class="co">#&gt;  [897] 0.6134946 0.2362237 0.6397636 0.7041491 0.2570834 0.1729661 0.1896267</span>
<span class="co">#&gt;  [904] 0.3168282 0.6134946 0.2597384 0.7746227 0.7746227 0.7746227 0.6397636</span>
<span class="co">#&gt;  [911] 0.7746227 0.1896267 0.2362237 0.2597384 0.7746227 0.1896267 0.6397636</span>
<span class="co">#&gt;  [918] 0.7746227 0.2570834 0.6397636 0.7746227 0.7746227 0.2362237 0.6397636</span>
<span class="co">#&gt;  [925] 0.2570834 0.8374954 0.2362237 0.7746227 0.2570834 0.6134946 0.7746227</span>
<span class="co">#&gt;  [932] 0.7544128 0.1896267 0.6134946 0.7746227 0.6397636 0.1519797 0.7544128</span>
<span class="co">#&gt;  [939] 0.8374954 0.3416221 0.2570834 0.6402906 0.7544128 0.8374954 0.7746227</span>
<span class="co">#&gt;  [946] 0.7544128 0.2362237 0.2362237 0.6134946 0.6397636 0.6397636 0.6134946</span>
<span class="co">#&gt;  [953] 0.6397636 0.7544128 0.1896267 0.7746227 0.8374954 0.8374954 0.7544128</span>
<span class="co">#&gt;  [960] 0.2570834 0.2570834 0.7274523 0.2387325 0.7544128 0.2597384 0.7746227</span>
<span class="co">#&gt;  [967] 0.2597384 0.1896267 0.7746227 0.7269985 0.6134946 0.2570834 0.6397636</span>
<span class="co">#&gt;  [974] 0.7746227 0.6397636 0.7269985 0.1896267 0.1896267 0.6397636 0.8374954</span>
<span class="co">#&gt;  [981] 0.7041491 0.7269985 0.1729661 0.8374954 0.2570834 0.6397636 0.7746227</span>
<span class="co">#&gt;  [988] 0.6397636 0.6397636 0.8374954 0.2570834 0.2362237 0.6134946 0.8374954</span>
<span class="co">#&gt;  [995] 0.1729661 0.8374954 0.6134946 0.2362237 0.7746227 0.7544128</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; $linear.predictors</span>
<span class="co">#&gt;    [1] -1.56475023  1.23460032  0.97944724  0.97944724  0.46202560  1.63970937</span>
<span class="co">#&gt;    [7]  0.86713465  0.46202560 -1.45243763 -1.56475023 -0.08363644  1.52739678</span>
<span class="co">#&gt;   [13] -1.06118299  0.97944724  0.46202560  1.23460032  1.23460032 -0.76838654</span>
<span class="co">#&gt;   [19] -1.45243763 -1.17349559  0.97944724 -1.56475023  0.46202560  1.12228773</span>
<span class="co">#&gt;   [25]  0.57433819  1.23460032  1.23460032 -1.56475023  0.57433819  0.46202560</span>
<span class="co">#&gt;   [31] -1.56475023  1.23460032  1.63970937  1.12228773  0.86713465 -1.31404858</span>
<span class="co">#&gt;   [37]  0.57433819  0.57433819  0.98173473  0.46202560 -1.45243763 -1.45243763</span>
<span class="co">#&gt;   [43]  1.23460032  0.46202560  0.57433819  0.57433819  1.52739678 -0.65607394</span>
<span class="co">#&gt;   [49]  0.57433819 -1.06118299  0.57433819 -1.45243763  1.12228773  0.57433819</span>
<span class="co">#&gt;   [55]  1.23460032  1.23460032 -1.06118299  0.57433819 -1.17349559  1.23460032</span>
<span class="co">#&gt;   [61] -1.45243763  0.46202560 -1.45243763 -1.04732858 -0.65607394 -1.71915763</span>
<span class="co">#&gt;   [67]  1.63970937  1.12228773  1.63970937 -1.06118299 -1.56475023  0.57433819</span>
<span class="co">#&gt;   [73]  0.97944724 -1.06118299 -1.45243763 -1.06118299  1.63970937 -1.04732858</span>
<span class="co">#&gt;   [79]  1.23460032 -1.45243763  1.12228773 -1.17349559  0.97944724 -1.56475023</span>
<span class="co">#&gt;   [85]  0.57433819  1.23460032  1.23460032 -1.06118299  1.23460032 -1.06118299</span>
<span class="co">#&gt;   [91] -1.06118299  1.12228773 -1.56475023 -1.45243763  1.63970937  1.23460032</span>
<span class="co">#&gt;   [97]  0.97944724  1.12228773 -1.06118299  1.63970937 -1.06118299  1.23460032</span>
<span class="co">#&gt;  [103]  0.97944724  0.46202560  1.23460032 -0.65607394 -1.06118299  1.23460032</span>
<span class="co">#&gt;  [109] -1.06118299 -1.04732858  1.23460032  0.46202560 -1.06118299  1.23460032</span>
<span class="co">#&gt;  [115]  0.57433819  0.97944724  0.46202560  1.23460032  1.23460032 -1.04732858</span>
<span class="co">#&gt;  [121]  1.23460032  1.23460032  0.46202560 -1.45243763  1.12228773  0.46202560</span>
<span class="co">#&gt;  [127]  1.12228773  0.86713465 -1.56475023 -1.06118299  1.23460032 -1.45243763</span>
<span class="co">#&gt;  [133]  1.23460032  1.52739678 -1.17349559  1.23460032 -1.06118299  1.63970937</span>
<span class="co">#&gt;  [139]  1.23460032 -1.06118299  1.23460032  0.46202560  1.23460032 -2.11041227</span>
<span class="co">#&gt;  [145] -1.06118299  1.23460032 -2.11041227 -1.56475023 -1.56475023 -1.06118299</span>
<span class="co">#&gt;  [151]  1.12228773  1.52739678  1.23460032  0.46202560 -1.06118299  1.23460032</span>
<span class="co">#&gt;  [157] -1.45243763  0.46202560  0.46202560  0.46202560  1.12228773 -0.65607394</span>
<span class="co">#&gt;  [163] -1.56475023  0.57433819 -1.06118299  1.52739678 -1.06118299  0.57433819</span>
<span class="co">#&gt;  [169]  0.57433819  0.57433819  0.46202560  0.46202560  0.57433819  1.23460032</span>
<span class="co">#&gt;  [175]  0.97944724  0.46202560  1.52739678  0.46202560 -1.45243763 -1.04732858</span>
<span class="co">#&gt;  [181] -1.45243763  0.57433819 -1.06118299 -2.11041227  1.63970937  1.63970937</span>
<span class="co">#&gt;  [187] -1.56475023  1.63970937 -1.45243763  0.57433819 -1.17349559 -0.76838654</span>
<span class="co">#&gt;  [193]  1.23460032  1.23460032 -1.45243763 -0.65607394  1.12228773  0.57433819</span>
<span class="co">#&gt;  [199] -1.06118299  1.23460032  1.63970937  1.63970937  1.23460032  1.52739678</span>
<span class="co">#&gt;  [205]  0.57433819  0.57433819  1.52739678 -1.17349559  1.12228773 -1.06118299</span>
<span class="co">#&gt;  [211]  0.46202560  0.57433819 -1.06118299  0.57433819  1.23460032 -1.06118299</span>
<span class="co">#&gt;  [217] -1.06118299  1.12228773 -1.15964118  1.12228773 -1.45243763  1.23460032</span>
<span class="co">#&gt;  [223] -1.15964118 -1.45243763 -0.65607394 -1.45243763 -1.17349559  0.46202560</span>
<span class="co">#&gt;  [229]  1.23460032  1.23460032 -1.06118299  1.12228773 -1.71915763  1.63970937</span>
<span class="co">#&gt;  [235]  1.63970937 -1.15964118  0.57433819  1.23460032  0.57433819  0.46202560</span>
<span class="co">#&gt;  [241]  1.12228773  0.57433819 -1.17349559 -1.06118299  0.46202560  1.23460032</span>
<span class="co">#&gt;  [247] -1.45243763 -0.08363644  0.57433819  1.23460032  0.57433819  1.23460032</span>
<span class="co">#&gt;  [253]  1.23460032  0.97944724  0.57433819  1.12228773  1.23460032  1.63970937</span>
<span class="co">#&gt;  [259]  1.12228773  0.57433819 -1.56475023  1.12228773  0.57433819 -1.45243763</span>
<span class="co">#&gt;  [265]  1.12228773 -1.06118299 -0.08363644  1.23460032  0.57433819 -1.04732858</span>
<span class="co">#&gt;  [271]  1.23460032  1.23460032  1.52739678  1.23460032  1.12228773 -1.06118299</span>
<span class="co">#&gt;  [277] -1.17349559  1.63970937  0.46202560  0.46202560  0.57433819 -1.06118299</span>
<span class="co">#&gt;  [283] -1.04732858  0.57433819  1.12228773  1.23460032  1.12228773 -0.76838654</span>
<span class="co">#&gt;  [289]  0.57433819  0.46202560 -0.08363644  0.57433819  1.23460032  0.46202560</span>
<span class="co">#&gt;  [295]  1.23460032 -1.06118299  0.46202560  1.23460032  1.63970937 -1.56475023</span>
<span class="co">#&gt;  [301]  1.23460032  1.12228773  0.57433819 -1.45243763  1.23460032  0.97944724</span>
<span class="co">#&gt;  [307]  1.23460032  1.63970937  1.23460032 -1.17349559 -1.06118299 -1.06118299</span>
<span class="co">#&gt;  [313] -1.06118299  0.46202560  1.23460032  0.46202560  0.97944724  0.46202560</span>
<span class="co">#&gt;  [319]  1.52739678  0.57433819  1.12228773  1.23460032  1.63970937  0.46202560</span>
<span class="co">#&gt;  [325]  0.57433819 -1.31404858  1.63970937  0.57433819 -1.06118299 -0.76838654</span>
<span class="co">#&gt;  [331] -0.65607394  1.12228773 -0.65607394  1.23460032 -1.04732858  1.12228773</span>
<span class="co">#&gt;  [337]  1.12228773  0.46202560  0.57433819 -1.56475023  1.12228773  1.23460032</span>
<span class="co">#&gt;  [343] -1.04732858 -1.04732858  1.23460032 -1.17349559  0.86713465  1.23460032</span>
<span class="co">#&gt;  [349] -1.31404858 -1.17349559  0.57433819  0.46202560  1.63970937  0.46202560</span>
<span class="co">#&gt;  [355]  1.23460032  1.23460032 -1.45243763  0.46202560 -1.45243763  1.63970937</span>
<span class="co">#&gt;  [361]  1.52739678 -1.45243763  0.46202560  1.12228773  0.46202560 -1.17349559</span>
<span class="co">#&gt;  [367]  1.63970937 -1.45243763 -1.06118299 -1.45243763 -0.65607394  0.46202560</span>
<span class="co">#&gt;  [373] -1.06118299  0.57433819  1.52739678 -1.06118299  1.12228773  0.97944724</span>
<span class="co">#&gt;  [379]  1.12228773  0.97944724 -1.06118299  0.57433819 -1.06118299  1.12228773</span>
<span class="co">#&gt;  [385] -1.56475023  0.46202560 -1.56475023  1.23460032 -1.45243763  1.23460032</span>
<span class="co">#&gt;  [391] -2.11041227  0.57433819  1.23460032  0.97944724  0.46202560 -1.06118299</span>
<span class="co">#&gt;  [397]  1.12228773  0.97944724 -1.45243763  0.97944724 -1.45243763  1.63970937</span>
<span class="co">#&gt;  [403]  1.23460032  0.97944724 -1.06118299 -0.65607394  1.12228773  1.23460032</span>
<span class="co">#&gt;  [409] -1.06118299 -0.08363644  0.46202560 -1.56475023 -1.45243763  0.57433819</span>
<span class="co">#&gt;  [415]  0.46202560 -1.06118299  1.52739678  1.23460032  1.23460032 -0.65607394</span>
<span class="co">#&gt;  [421]  1.52739678 -1.45243763 -1.17349559  1.12228773  0.46202560  1.12228773</span>
<span class="co">#&gt;  [427] -1.56475023 -0.65607394  0.57433819 -1.45243763  0.46202560 -1.06118299</span>
<span class="co">#&gt;  [433] -1.17349559 -0.76838654  0.57433819 -1.06118299  1.23460032 -1.06118299</span>
<span class="co">#&gt;  [439]  0.57433819  0.97944724  1.23460032  0.46202560  1.12228773 -1.06118299</span>
<span class="co">#&gt;  [445] -1.06118299  1.23460032  1.23460032  0.57433819  1.23460032 -1.17349559</span>
<span class="co">#&gt;  [451]  0.57433819  1.23460032 -1.56475023  0.46202560  1.23460032  0.46202560</span>
<span class="co">#&gt;  [457]  0.57433819  0.46202560  1.12228773  1.12228773  1.23460032 -1.45243763</span>
<span class="co">#&gt;  [463]  1.23460032 -1.06118299 -1.56475023 -0.76838654 -1.06118299  0.46202560</span>
<span class="co">#&gt;  [469]  1.23460032  1.23460032  1.63970937 -0.76838654 -1.17349559  0.57433819</span>
<span class="co">#&gt;  [475] -1.06118299  0.57433819  1.63970937  0.57433819  0.46202560 -1.04732858</span>
<span class="co">#&gt;  [481] -0.65607394 -1.45243763  1.63970937  1.63970937  0.46202560  1.52739678</span>
<span class="co">#&gt;  [487] -1.06118299  1.63970937  1.23460032  0.46202560 -1.45243763  0.57433819</span>
<span class="co">#&gt;  [493]  0.57662568 -0.65607394  1.23460032  0.57433819 -1.45243763  1.12228773</span>
<span class="co">#&gt;  [499]  1.23460032 -1.06118299  0.46202560  1.63970937 -0.65607394 -1.45243763</span>
<span class="co">#&gt;  [505] -1.06118299 -1.17349559  0.57433819  1.12228773  0.57433819  1.23460032</span>
<span class="co">#&gt;  [511]  1.12228773 -1.06118299 -1.56475023  0.46202560  0.57433819  1.12228773</span>
<span class="co">#&gt;  [517]  0.57433819  1.23460032  0.57433819  1.52739678  0.46202560  1.23460032</span>
<span class="co">#&gt;  [523]  0.46202560  0.57433819 -0.08363644 -1.17349559  0.57433819  0.46202560</span>
<span class="co">#&gt;  [529] -1.17349559  1.23460032  1.23460032  1.52739678  1.23460032 -1.06118299</span>
<span class="co">#&gt;  [535]  1.63970937  0.46202560  0.46202560  0.86713465  1.23460032 -1.17349559</span>
<span class="co">#&gt;  [541]  0.97944724  1.23460032  1.23460032  1.23460032 -1.06118299 -1.06118299</span>
<span class="co">#&gt;  [547] -0.76838654  0.57433819  1.23460032  1.63970937  0.97944724 -0.76838654</span>
<span class="co">#&gt;  [553]  1.23460032  0.97944724 -1.06118299  0.57433819  1.12228773 -1.06118299</span>
<span class="co">#&gt;  [559]  0.57433819 -0.76838654  0.57433819  0.57433819  1.23460032 -1.04732858</span>
<span class="co">#&gt;  [565]  0.57433819  0.57433819  1.23460032  1.52739678  1.63970937 -0.65607394</span>
<span class="co">#&gt;  [571] -0.08363644 -1.04732858  0.57433819 -1.45243763 -1.17349559 -1.56475023</span>
<span class="co">#&gt;  [577] -1.06118299  0.57433819  1.23460032  0.57433819 -1.06118299  1.12228773</span>
<span class="co">#&gt;  [583]  0.97944724 -1.04732858  0.57433819  0.57433819  1.52739678  1.63970937</span>
<span class="co">#&gt;  [589]  1.23460032  0.57433819  1.12228773  0.97944724  0.57433819 -1.04732858</span>
<span class="co">#&gt;  [595]  0.46202560  0.97944724  1.12228773  1.23460032 -1.06118299 -1.06118299</span>
<span class="co">#&gt;  [601]  0.57433819  0.97944724  1.12228773 -1.45243763 -1.17349559  1.52739678</span>
<span class="co">#&gt;  [607]  0.97944724  1.23460032  1.23460032  0.86713465 -0.65607394  0.57433819</span>
<span class="co">#&gt;  [613] -1.06118299  0.57433819  1.12228773  0.57433819  1.12228773  0.46202560</span>
<span class="co">#&gt;  [619]  0.46202560  0.46202560  1.12228773  0.46202560 -1.15964118  1.23460032</span>
<span class="co">#&gt;  [625] -0.65607394  1.23460032 -1.17349559 -1.45243763  1.23460032  1.23460032</span>
<span class="co">#&gt;  [631]  1.23460032  0.57433819 -0.08363644 -1.06118299 -1.06118299  0.86713465</span>
<span class="co">#&gt;  [637] -1.56475023  0.46202560  1.23460032  0.57433819  1.23460032  1.12228773</span>
<span class="co">#&gt;  [643]  0.57433819  1.12228773 -1.17349559  1.23460032 -1.06118299 -0.08363644</span>
<span class="co">#&gt;  [649]  1.63970937 -1.04732858 -1.06118299 -1.06118299  0.57433819  1.12228773</span>
<span class="co">#&gt;  [655] -0.65607394  1.12228773  0.57433819  1.23460032  1.23460032 -1.06118299</span>
<span class="co">#&gt;  [661] -1.06118299  1.63970937 -1.06118299 -1.04732858 -1.17349559  0.46202560</span>
<span class="co">#&gt;  [667]  0.57662568  0.57433819 -0.76838654  1.12228773 -1.17349559  1.12228773</span>
<span class="co">#&gt;  [673] -1.17349559  0.86713465  0.57433819  0.57433819  1.23460032  0.57433819</span>
<span class="co">#&gt;  [679]  1.23460032  0.57433819  0.46202560 -0.65607394  1.12228773  1.63970937</span>
<span class="co">#&gt;  [685]  1.52739678  1.23460032  0.57433819  0.57433819  0.57433819  1.12228773</span>
<span class="co">#&gt;  [691] -1.17349559  0.57433819  1.23460032  1.23460032  0.86713465  0.46202560</span>
<span class="co">#&gt;  [697]  0.46202560  0.97944724 -0.76838654 -1.45243763 -1.06118299  0.97944724</span>
<span class="co">#&gt;  [703]  1.23460032  1.12228773 -1.17349559  1.23460032  1.23460032  1.12228773</span>
<span class="co">#&gt;  [709]  0.57433819 -1.71915763  0.57433819  1.12228773  1.52739678 -1.56475023</span>
<span class="co">#&gt;  [715] -1.06118299  0.97944724  1.63970937  0.57433819  0.57433819  0.57433819</span>
<span class="co">#&gt;  [721]  0.46202560  1.23460032 -1.06118299  1.23460032 -1.06118299  0.97944724</span>
<span class="co">#&gt;  [727]  0.57433819  0.46202560  1.12228773  0.57433819 -1.06118299  1.63970937</span>
<span class="co">#&gt;  [733] -1.04732858 -1.06118299  0.46202560  0.46202560  0.46202560  1.12228773</span>
<span class="co">#&gt;  [739] -0.76838654 -1.06118299 -0.76838654  0.57433819 -1.06118299  1.23460032</span>
<span class="co">#&gt;  [745] -0.76838654  1.23460032 -1.56475023  1.23460032  1.23460032  0.97944724</span>
<span class="co">#&gt;  [751] -1.45243763  1.23460032  1.23460032 -1.04732858  1.23460032 -1.45243763</span>
<span class="co">#&gt;  [757]  1.12228773  0.57662568  1.23460032 -1.45243763 -1.06118299  0.57433819</span>
<span class="co">#&gt;  [763] -1.06118299  1.23460032  0.97944724  0.97944724 -0.65607394  0.46202560</span>
<span class="co">#&gt;  [769] -1.45243763  0.57433819  1.23460032  0.57433819 -1.06118299  0.57433819</span>
<span class="co">#&gt;  [775] -0.76838654  1.12228773 -1.45243763 -0.65607394 -1.06118299  1.23460032</span>
<span class="co">#&gt;  [781] -1.06118299  1.23460032 -0.65607394  0.57433819  1.23460032  0.57433819</span>
<span class="co">#&gt;  [787] -1.04732858 -0.08363644  1.23460032  0.57433819  0.46202560 -1.56475023</span>
<span class="co">#&gt;  [793]  1.23460032  1.12228773  1.23460032  0.57433819  1.23460032  0.57433819</span>
<span class="co">#&gt;  [799]  1.23460032  0.57433819  1.23460032 -1.06118299 -1.45243763  0.46202560</span>
<span class="co">#&gt;  [805]  1.23460032  0.57433819 -1.71915763 -1.06118299 -2.11041227  1.23460032</span>
<span class="co">#&gt;  [811] -1.06118299 -1.06118299 -1.06118299  0.97944724 -1.45243763 -0.65607394</span>
<span class="co">#&gt;  [817]  0.97944724 -1.06118299  0.57662568  0.98173473 -1.56475023  1.23460032</span>
<span class="co">#&gt;  [823]  0.57433819  1.23460032  0.46202560  1.63970937  0.57433819 -1.06118299</span>
<span class="co">#&gt;  [829]  1.23460032 -0.76838654 -1.06118299  1.12228773  1.23460032 -0.65607394</span>
<span class="co">#&gt;  [835]  0.57433819 -1.06118299 -1.15964118  0.57433819  1.23460032  0.57433819</span>
<span class="co">#&gt;  [841] -0.08363644  1.23460032  0.57433819 -1.17349559  0.46202560  0.57433819</span>
<span class="co">#&gt;  [847] -1.45243763  1.63970937  1.23460032 -1.56475023  0.57433819  1.23460032</span>
<span class="co">#&gt;  [853]  0.46202560  0.97944724  1.23460032  0.97944724  1.23460032 -2.11041227</span>
<span class="co">#&gt;  [859]  1.12228773 -1.56475023  1.12228773  1.63970937  1.23460032  1.23460032</span>
<span class="co">#&gt;  [865]  0.57433819  1.23460032  1.63970937  0.97944724 -0.65607394  1.23460032</span>
<span class="co">#&gt;  [871] -1.45243763  1.23460032  0.46202560 -0.76838654  0.46202560  1.23460032</span>
<span class="co">#&gt;  [877] -1.06118299  0.57433819  0.97944724 -1.15964118 -1.45243763 -1.06118299</span>
<span class="co">#&gt;  [883] -1.71915763 -1.71915763  0.57433819  0.46202560 -1.17349559  1.63970937</span>
<span class="co">#&gt;  [889] -1.06118299  1.23460032  1.23460032 -1.17349559 -1.17349559  1.52739678</span>
<span class="co">#&gt;  [895] -0.65607394  0.46202560  0.46202560 -1.17349559  0.57433819  0.86713465</span>
<span class="co">#&gt;  [901] -1.06118299 -1.56475023 -1.45243763 -0.76838654  0.46202560 -1.04732858</span>
<span class="co">#&gt;  [907]  1.23460032  1.23460032  1.23460032  0.57433819  1.23460032 -1.45243763</span>
<span class="co">#&gt;  [913] -1.17349559 -1.04732858  1.23460032 -1.45243763  0.57433819  1.23460032</span>
<span class="co">#&gt;  [919] -1.06118299  0.57433819  1.23460032  1.23460032 -1.17349559  0.57433819</span>
<span class="co">#&gt;  [925] -1.06118299  1.63970937 -1.17349559  1.23460032 -1.06118299  0.46202560</span>
<span class="co">#&gt;  [931]  1.23460032  1.12228773 -1.45243763  0.46202560  1.23460032  0.57433819</span>
<span class="co">#&gt;  [937] -1.71915763  1.12228773  1.63970937 -0.65607394 -1.06118299  0.57662568</span>
<span class="co">#&gt;  [943]  1.12228773  1.63970937  1.23460032  1.12228773 -1.17349559 -1.17349559</span>
<span class="co">#&gt;  [949]  0.46202560  0.57433819  0.57433819  0.46202560  0.57433819  1.12228773</span>
<span class="co">#&gt;  [955] -1.45243763  1.23460032  1.63970937  1.63970937  1.12228773 -1.06118299</span>
<span class="co">#&gt;  [961] -1.06118299  0.98173473 -1.15964118  1.12228773 -1.04732858  1.23460032</span>
<span class="co">#&gt;  [967] -1.04732858 -1.45243763  1.23460032  0.97944724  0.46202560 -1.06118299</span>
<span class="co">#&gt;  [973]  0.57433819  1.23460032  0.57433819  0.97944724 -1.45243763 -1.45243763</span>
<span class="co">#&gt;  [979]  0.57433819  1.63970937  0.86713465  0.97944724 -1.56475023  1.63970937</span>
<span class="co">#&gt;  [985] -1.06118299  0.57433819  1.23460032  0.57433819  0.57433819  1.63970937</span>
<span class="co">#&gt;  [991] -1.06118299 -1.17349559  0.46202560  1.63970937 -1.56475023  1.63970937</span>
<span class="co">#&gt;  [997]  0.46202560 -1.17349559  1.23460032  1.12228773</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; $loglikelihood</span>
<span class="co">#&gt; [1] -567.9048</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; $converged</span>
<span class="co">#&gt; [1] TRUE</span></pre></body></html></div>
</div>
<div id="the-discretize-method" class="section level2">
<h2 class="hasAnchor">
<a href="#the-discretize-method" class="anchor"></a>The <code>discretize</code> method</h2>
<p>The <code>discretize</code> method allows the user to discretize a new “raw” input set by applying the best discretization scheme found by the <code>glmdisc</code> function in a provided <code>glmdisc</code> object.</p>
<div class="sourceCode" id="cb16"><html><body><pre class="r"><span class="no">x_new</span> <span class="kw">&lt;-</span> <span class="fu"><a href="../reference/discretize.html">discretize</a></span>(<span class="no">discretization</span>,<span class="no">x</span>)</pre></body></html></div>
<table class="table"><tbody>
<tr class="odd">
<td align="left">1</td>
<td align="left">3</td>
<td align="left">4</td>
</tr>
<tr class="even">
<td align="left">2</td>
<td align="left">3</td>
<td align="left">1</td>
</tr>
<tr class="odd">
<td align="left">5</td>
<td align="left">4</td>
<td align="left">1</td>
</tr>
<tr class="even">
<td align="left">5</td>
<td align="left">4</td>
<td align="left">1</td>
</tr>
<tr class="odd">
<td align="left">5</td>
<td align="left">3</td>
<td align="left">4</td>
</tr>
<tr class="even">
<td align="left">2</td>
<td align="left">4</td>
<td align="left">1</td>
</tr>
</tbody></table>
</div>
<div id="the-predict-method" class="section level2">
<h2 class="hasAnchor">
<a href="#the-predict-method" class="anchor"></a>The <code>predict</code> method</h2>
<p>The <code>discretize</code> method allows the user to predict the response of a new “raw” input set by first discretizing it using the previously described <code>discretize</code> method and a previously trained <code>glmdisc</code> object. It then predicts using the standard <code>predict.glm</code> method and the best discretization scheme found by the <code>glmdisc</code> function.</p>
<div class="sourceCode" id="cb17"><html><body><pre class="r"># pred_new &lt;- predict(discretization, data.frame(x))</pre></body></html></div>
</div>
</div>
<div id="future-developments" class="section level1">
<h1 class="hasAnchor">
<a href="#future-developments" class="anchor"></a>Future developments</h1>
<div id="enhancing-the-discretization-package" class="section level2">
<h2 class="hasAnchor">
<a href="#enhancing-the-discretization-package" class="anchor"></a>Enhancing the <code>discretization</code> package</h2>
<p>The <code>discretization</code> package available from CRAN implements some existing supervised univariate discretization methods (applicable only to continuous inputs) : ChiMerge (<code>chiM</code>), Chi2 (<code>chi2</code>), Extended Chi2 (<code>extendChi2</code>), Modified Chi2 (<code>modChi2</code>), MDLP (<code>mdlp</code>), CAIM, CACC and AMEVA (<code>disc.Topdown</code>).</p>
<p>To compare the result of such methods to the one we propose, the package <code>discretization</code> will be enhanced to support missing values (by putting them in a single class for example) and quantitative features (through a Chi2 grouping method for example).</p>
<!-- ## Integration of interaction discovery -->
<!-- Very often, predictive features $X$ "interact" with each other with respect to the response feature. This is classical in the context of Credit Scoring or biostatistics (only the simultaneous presence of several features - genes, SNP, etc. is predictive of a disease). -->
<!-- With the growing number of potential predictors and the time required to manually analyze if an interaction should be added or not, there is a strong need for automatic procedures that screen potential interaction variables. This will be the subject of future work. -->
</div>
<div id="possibility-of-changing-model-assumptions" class="section level2">
<h2 class="hasAnchor">
<a href="#possibility-of-changing-model-assumptions" class="anchor"></a>Possibility of changing model assumptions</h2>
<p>In the third section, we described two fundamental modelling hypotheses that were made:</p>
<ul>
<li><p>The real probability density function <span class="math inline">\(p(Y|X)\)</span> can be approximated by a logistic regression <span class="math inline">\(p_\theta(Y|Q)\)</span> on the discretized data <span class="math inline">\(Q\)</span>.</p></li>
<li><p>The nature of the relationship of <span class="math inline">\(Q_j\)</span> to <span class="math inline">\(X_j\)</span> is:</p></li>
<li><p>A polytomous logistic regression if <span class="math inline">\(X_j\)</span> is continuous;</p></li>
<li><p>A contengency table if <span class="math inline">\(X_j\)</span> is qualitative.</p></li>
</ul>
<p>These hypotheses are “building blocks” that could be changed at the modeller’s will: discretization could optimize other models.</p>
<!-- ## Make it faster -->
<!-- The current implementation is heavily based on R code. A lot of logistic regression are fit, either through `multinom`, `polr` or `glm` calls, which can probably be sped up by an intelligent initialization of their parameters and a limitation or their number of iterations. -->
<!-- We also loop of qualitative features and their values, which is known to be slow in R. A C++ version of a Gibbs sampler (which is used here) described by Hadley Wickham in [Advanced R](http://adv-r.had.co.nz/Rcpp.html#rcpp-case-studies) shows a potential speed up by 40. -->
</div>
</div>
<div id="references" class="section level1">
<h1 class="hasAnchor">
<a href="#references" class="anchor"></a>References</h1>
<p>Celeux, G., Chauveau, D., Diebolt, J. (1995), On Stochastic Versions of the EM Algorithm. [Research Report] RR-2514, INRIA. 1995. <inria-00074164></inria-00074164></p>
<p>Agresti, A. (2002) <strong>Categorical Data</strong>. Second edition. Wiley.</p>
<p>Ramírez‐Gallego, S., García, S., Mouriño‐Talín, H., Martínez‐Rego, D., Bolón‐Canedo, V., Alonso‐Betanzos, A. and Herrera, F. (2016). Data discretization: taxonomy and big data challenge. <em>Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery</em>, 6(1), 5-21.</p>
<p>Geman, S., &amp; Geman, D. (1987). Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images. In <em>Readings in Computer Vision</em> (pp. 564-584).</p>
<p>Hastings, W. K. (1970). Monte Carlo sampling methods using Markov chains and their applications. <em>Biometrika</em>, 57(1), 97-109.</p>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

        <nav id="toc" data-toggle="toc"><h2 data-toc-skip>Contents</h2>
    </nav>
</div>

</div>



      <footer><div class="copyright">
  <p>Developed by Adrien Ehrhardt, Vincent Vandewalle.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="https://pkgdown.r-lib.org/">pkgdown</a> 1.5.1.</p>
</div>

      </footer>
</div>

  


  </body>
</html>
